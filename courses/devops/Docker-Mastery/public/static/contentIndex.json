{"Docker-Cheatsheet":{"slug":"Docker-Cheatsheet","filePath":"Docker Cheatsheet.md","title":"Docker Cheatsheet","links":[],"tags":[],"content":"Docker Cheatsheet – Developer &amp; DevOps Reference\nThis page provides a concise, command-focused reference for common Docker tasks\nused by developers and DevOps engineers. It is intended for quick lookup during\ndevelopment, debugging, and operational troubleshooting.\nThe commands are grouped by use case and reflect practical, real-world workflows\nrather than exhaustive coverage.\nImages\n\nList images: docker images\nPull image: docker pull nginx:1.25-alpine\nBuild image: docker build -t myorg/app:1.0.0 .\nBuild no cache: docker build --no-cache -t myorg/app:1.0.0 .\nTag image: docker tag myorg/app:1.0.0 myorg/app:latest\nPush image: docker push myorg/app:1.0.0\nInspect image: docker inspect myorg/app:1.0.0\nImage history: docker history myorg/app:1.0.0\nPrune unused images: docker image prune -f\nPrune all unused: docker image prune -a -f\n\nContainers – Run &amp; Lifecycle\n\nRun interactive: docker run --rm -it alpine:3.19 sh\nRun detached + port + name: docker run -d --name web -p 8080:80 nginx:1.25-alpine\nRun with limits: docker run -d --name web -p 8080:80 --cpus=&quot;1&quot; --memory=&quot;512m&quot; nginx:1.25-alpine\nRun with restart: docker run -d --restart=on-failure --name api -p 8080:8080 myorg/api:1.0.0\nList running: docker ps\nList all: docker ps -a\nStop container: docker stop web\nStart container: docker start web\nRemove container: docker rm web\nLogs follow: docker logs -f web\nExec shell: docker exec -it web sh\nInspect container: docker inspect web\nResource stats: docker stats\n\nNetworking\n\nList networks: docker network ls\nCreate bridge network: docker network create app-net\nRun on network (db):\ndocker run -d --name db --network app-net -e POSTGRES_PASSWORD=secret postgres:15-alpine\nRun on network (api):\ndocker run -d --name api --network app-net -e DB_HOST=db myorg/api:1.0.0\nConnect existing container: docker network connect app-net api\nInspect network: docker network inspect app-net\nHost network: docker run --net=host nginx:1.25-alpine\nNone network: docker run --network none alpine:3.19\n\nVolumes &amp; Persistence\n\nCreate volume: docker volume create pgdata\nList volumes: docker volume ls\nInspect volume: docker volume inspect pgdata\nPrune volumes: docker volume prune -f\nNamed volume (Postgres):\ndocker run -d --name db -e POSTGRES_PASSWORD=secret -v pgdata:/var/lib/postgresql/data postgres:15-alpine\nBind mount (dev):\ndocker run -d --name web-dev -p 3000:3000 -v &quot;$PWD/src:/usr/src/app&quot; node:22-alpine sh -c &quot;cd /usr/src/app &amp;&amp; npm install &amp;&amp; npm run dev&quot;\n\nDockerfile – Core Snippets\n\nBase image: FROM eclipse-temurin:21-jre-alpine\nRun command: RUN apk add --no-cache curl\nCopy files: COPY app.jar /app/app.jar\nWorkdir: WORKDIR /app\nEnv var: ENV SPRING_PROFILES_ACTIVE=prod\nBuild‑time arg:\nARG BUILD_VERSION=dev\nLABEL version=&quot;${BUILD_VERSION}&quot;\nExpose port: EXPOSE 8080\nNon‑root user:\nRUN addgroup -S app &amp;&amp; adduser -S app -G app\nUSER app\nEntrypoint + cmd:\nENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\nCMD [&quot;--spring.profiles.active=prod&quot;]\nHealthcheck:\nHEALTHCHECK --interval=30s --timeout=3s CMD curl -f http://localhost:8080/actuator/health || exit 1\n\nMulti‑Stage Builds\n\n\nNode example:\n   ```\n   FROM node:22-alpine AS build\n   WORKDIR /app\n   COPY package*.json ./\n   RUN npm ci\n   COPY . .\n   RUN npm run build\n\n   FROM nginx:1.25-alpine\n   COPY --from=build /app/dist /usr/share/nginx/html\n   EXPOSE 80\n   CMD [&quot;nginx&quot;,&quot;-g&quot;,&quot;daemon off;&quot;]\n\n   ```\n\n\n\nSecurity Essentials\n\nDrop root in Dockerfile: USER app (after creating user)\nDrop all caps, add minimal:\ndocker run -d --name web --cap-drop=ALL --cap-add=NET_BIND_SERVICE -p 80:80 nginx:1.25-alpine\nRead‑only FS:\ndocker run -d --name api --read-only -v app-tmp:/tmp myorg/api:1.0.0\n\nRegistries &amp; Tagging\n\nLogin: docker login my-registry.example.com\nTag for registry:\ndocker tag myorg/api:1.0.0 my-registry.example.com/myorg/api:1.0.0\nPush: docker push my-registry.example.com/myorg/api:1.0.0\n\nDocker Compose – Essentials\n\n\ndocker-compose.yml minimal:\n   ```\n   services:\n   db:\n      image: postgres:15-alpine\n      environment:\n         POSTGRES_PASSWORD: secret\n      volumes:\n         - pgdata:/var/lib/postgresql/data\n   api:\n      image: myorg/orders-api:1.0.0\n      ports:\n         - &quot;8080:8080&quot;\n      environment:\n         DB_HOST: db\n   volumes:\n   pgdata:\n\n   ```\n\n\n\nBring up: docker compose up -d\n\n\nStatus: docker compose ps\n\n\nLogs: docker compose logs -f api\n\n\nTear down: docker compose down\n\n\nTroubleshooting Quickies\n\nShell into container: docker exec -it api sh\nCheck env: docker inspect api\nCheck mounts: docker inspect api\nCheck ports: docker ps\nTest from host: curl http://localhost:8080/actuator/health\n"},"Docker/0-Course-Overview-and-Index":{"slug":"Docker/0-Course-Overview-and-Index","filePath":"Docker/0 Course Overview and Index.md","title":"0. Course Overview and Index","links":["Docker/1-Docker-Images","Docker/2-Docker-Containers","Docker/3-Docker-Networking","Docker/4-Docker-Volumes","Docker/5-Dockerfile-Mastery","Docker/6-Multi‑Stage-Docker-Builds","Docker/7-Docker-Security-Essentials","Docker/8-Docker-Registries-and-Tagging","Docker/9-Docker-Compose","Docker/10-Debugging-Docker","Docker/11-From-Docker-to-Orchestrators"],"tags":[],"content":"This course builds a practical mental model of Docker from first principles and then connects it to real-world DevOps workflows.\nEach section below links to a focused note that you can read and practice independently.\n\n1. Docker Images – How the Lego Bricks of Containers Really Work\n( See: Docker Images )\nYou will learn:\n\n\nImage vs container vs registry\n\nImage as an immutable blueprint.\nContainer as a running process plus writable layer.\nRegistry as a Git‑like store for built images.\n\n\n\nLayered filesystems and why each instruction matters\n\nHow every Dockerfile instruction creates a new layer.\nWhy RUN apt-get update &amp;&amp; apt-get install ... vs multiple RUNs changes cache behavior.\nRefactoring a bad Dockerfile into a cache‑friendly one.\n\n\n\nBuild, tag, and push workflow\n\nMental model of docker build: context → Dockerfile → layers.\nSemantic tagging strategy: 1.0.0, 1.0.0-prod, latest as a pointer, not magic.\nEnd‑to‑end flow from local build to private registry push.\n\n\n\nInspecting and understanding images\n\nUsing docker inspect and docker history to reverse engineer an image.\nDetecting bloated layers (large COPY, uncleaned caches, logs, temp files).\nComparing openjdk vs temurin vs distroless images.\n\n\n\nCleaning up and managing the local image store\n\nWhy dangling images exist and when to prune safely.\nSafe vs dangerous cleanup commands with docker image prune and friends.\n\n\n\nPractical image best practices\n\nPrefer small, purpose‑built base images.\nKeep Dockerfile instructions deterministic and ordered.\nKeep secrets and credentials out of images.\n\n\n\n\n2. Containers – Processes, Not Tiny VMs\n( See: Docker Containers )\nYou will learn:\n\n\nMental model: container = process + isolation\n\nHigh‑level view of namespaces and cgroups.\nOperational differences between VMs and containers.\n\n\n\nContainer lifecycle in five phases\n\nCreate → Start → Running → Stopped → Removed.\nOne‑off task containers vs long‑running service containers (e.g., alpine vs nginx).\n\n\n\nRunning containers in real life\n\nThe docker run flags that actually matter: -d, --name, -p, -e, --restart, --rm.\nRunning a Spring Boot service with environment variables and a restart policy.\n\n\n\nMonitoring and interacting with containers\n\nUsing logs, exec, inspect, stats as your stethoscope toolkit.\nA step‑by‑step checklist for “App is not responding on port 8080”.\n\n\n\nResource control\n\nCPU and memory limits, and why they matter on multi‑tenant hosts.\nExamples of over‑limiting vs giving fair resource allocations.\n\n\n\nPractical rules of thumb\n\n“One main process per container” and when sidecars are acceptable.\nSensible restart policies for APIs vs cron‑like jobs.\n\n\n\n\n3. Docker Networking – Making Containers Talk Like Grown‑Ups\n( See: Docker Networking )\nYou will learn:\n\n\nMental model of container networking\n\nNetwork namespaces, veth pairs, and the default bridge.\nWhy localhost inside a container is not your host.\n\n\n\nBuilt‑in network drivers\n\nbridge (default), user‑defined bridge, host, none, and a high‑level view of overlay.\nWhen to use each driver and when to avoid it.\n\n\n\nUser‑defined bridges and Docker DNS\n\nHow Docker auto‑creates DNS names from container names.\nExample: db and api on app-net communicating via service names.\n\n\n\nPort publishing vs internal ports\n\nMental model for -p host:container mappings.\nCommon pitfalls: port already in use, wrong interface, host firewall issues.\n\n\n\nDebugging networking issues\n\nChecklist: container running, port exposed, port published, basic connectivity between containers.\nExample: stepwise debug of “API can’t connect to DB” on the same network.\n\n\n\nProduction considerations\n\nWhy --net=host is usually a bad idea.\nHow these networking concepts map cleanly to Kubernetes Services.\n\n\n\n\n4. Volumes – Keeping Data Alive When Containers Die\n( See: Docker Volumes )\nYou will learn:\n\n\nContainer filesystem vs persisted data\n\nWhy the writable container layer is ephemeral.\nWhere databases and uploaded files should actually live.\n\n\n\nTypes of Docker storage\n\nNamed volumes vs bind mounts vs tmpfs.\nPros and cons from both development and production perspectives.\n\n\n\nNamed volumes in practice\n\nUsing named volumes for Postgres, MySQL, MongoDB, etc.\nDemo: kill a DB container, start a new one, and show that data survives.\n\n\n\nBind mounts for fast developer feedback\n\nLive‑reloading workflows for Node/React/Angular or Spring Boot dev mode.\nRisks: permissions, line endings, OS semantics differences.\n\n\n\nInspecting and managing volumes\n\nListing volumes and finding where they live on disk.\nSimple backup and restore flows using tar or similar tools.\n\n\n\nProduction opinions\n\n“No important data in the container filesystem” rule.\nWhy arbitrary host path bind mounts are dangerous in production.\n\n\n\n\n5. Dockerfile Instructions – Writing Dockerfiles Like an Engineer\n(See: Dockerfile Mastery )\nYou will learn:\n\n\nDockerfile as a deterministic build recipe\n\nTop‑down execution, build cache, and layer invalidation.\nWhy instruction order directly affects build time and cache reuse.\n\n\n\nCore instructions, with intent\n\nFROM, RUN, COPY/ADD, WORKDIR, ENV, ARG, EXPOSE, USER, CMD, ENTRYPOINT, HEALTHCHECK.\nFor each: what it really means and when to use it.\n\n\n\nCOPY vs ADD\n\nWhy COPY is usually safer and more predictable.\nHow ADD’s extra features (remote URLs, automatic tar extract) can surprise you.\n\n\n\nENV and ARG patterns\n\nBuild‑time vs runtime configuration.\nExample pattern: version as an ARG, runtime profile as an ENV.\n\n\n\nCMD vs ENTRYPOINT\n\nFixed executable vs overridable parameters.\nPattern: ENTRYPOINT for the main binary, CMD for default arguments.\n\n\n\nSecurity‑aware Dockerfile writing\n\nCreating and switching to non‑root users.\nAvoiding secrets and sensitive files in images.\n\n\n\nRefactoring a bad Dockerfile\n\nStarting from a naive Dockerfile.\nIteratively improving size, cache behavior, and security.\n\n\n\n\n6. Multi‑Stage Builds – Shrinking Images and Attack Surface\n(See: Multi‑Stage Docker Builds )\nYou will learn:\n\n\nThe problem: fat images with build tools inside\n\nShipping JDKs, compilers, and dev dependencies to production.\nImpact on pull time, security posture, and disk usage.\n\n\n\nConcept: separate build stage from runtime stage\n\nHow AS build and COPY --from work.\nWhy multiple stages are cheap but powerful in Docker.\n\n\n\nLanguage‑specific examples\n\nNode: build static assets and serve from Nginx.\nJava: JDK + Maven for build, JRE‑only or distroless image for runtime.\nGo: build in golang, run in scratch or distroless.\n\n\n\nOptimizing build cache in multi‑stage builds\n\nPlacing dependency steps before copying full source.\nExample: separate pom.xml copy and mvn dependency:go-offline.\n\n\n\nSecurity and compliance benefits\n\nFewer binaries in the final image → smaller attack surface.\nCleaner vulnerability scan reports with less noise.\n\n\n\nMigration story\n\nTaking a legacy single‑stage Dockerfile.\nConverting it into a lean, multi‑stage build step by step.\n\n\n\n\n7. Docker Security – Baseline Guardrails for Devs and DevOps\n(See: Docker Security Essentials )\nYou will learn:\n\n\nThreat model for containers\n\nContainers share the host kernel and what that implies.\nWhy “it’s in a container” is not a security boundary by itself.\n\n\n\nUsers and permissions\n\nNon‑root users inside containers with the USER instruction.\nFile ownership patterns when using volumes and bind mounts.\n\n\n\nCapabilities and seccomp (conceptual)\n\nBasics of Linux capabilities and the principle of least privilege.\nSimple patterns: drop everything, then add back only what you need.\n\n\n\nImage hygiene\n\nKeeping base images updated and slim.\nInstalling only necessary packages and cleaning caches.\n\n\n\nRuntime hardening flags\n\nRead‑only root filesystems, no‑new‑privileges, and similar options.\nExample of tightening an Nginx container with runtime flags.\n\n\n\nSecrets handling basics\n\nWhy you never bake secrets into images.\nHigh‑level overview of secret managers and runtime injection.\n\n\n\n\n8. Registries and Tagging – Versioning Containers Like Real Software\n(See: Docker Registries and Tagging )\nYou will learn:\n\n\nRegistries as artifact repositories\n\nPublic registries (Docker Hub) vs private registries (ECR, GCR, Harbor, etc.).\nNamespaces and repository naming like myorg/service.\n\n\n\nTagging strategies that scale\n\nSemantic versions, build numbers, and Git SHA tags.\nWhen and how to use latest safely, especially outside production.\n\n\n\nPush/pull workflow in CI/CD\n\nBuild → tag with SHA + semantic version → push → deploy by tag.\nExample of a simple CI pipeline from commit to running container.\n\n\n\nPromoting images across environments\n\nRetag and redeploy the same artifact for dev, staging, prod.\nWhy this simplifies debugging, rollback, and auditing.\n\n\n\nRegistry access and authentication\n\nLogging in, using tokens and credentials securely.\nHigh‑level view of pulling from private registries in orchestrators.\n\n\n\n\n9. Docker Compose – Local Microservices Without Losing Your Mind\n(See: Docker Compose )\nYou will learn:\n\n\nWhy Docker Compose exists\n\nPain of long docker run commands for multi‑service stacks.\nYAML as a declarative “docker run on steroids”.\n\n\n\nCore Compose concepts\n\nServices, networks, and volumes defined in YAML.\nOne Compose file as a self‑contained local environment or stack.\n\n\n\nWalking through a simple stack\n\napi + db example with environment variables, ports, and volumes.\nHow Compose auto‑creates networks and DNS names.\n\n\n\nDeveloper workflows\n\nUsing docker compose up -d, logs -f, ps, down, restart.\nPattern: change code, rebuild image, restart only the affected service.\n\n\n\nPatterns and best practices\n\nOverride files for local development vs CI pipelines.\nUsing healthchecks to improve startup order and reliability.\n\n\n\nBridge to Kubernetes\n\nMapping services, networks, and volumes to Deployments and Services.\nUsing Compose mental models as a stepping stone to k8s.\n\n\n\n\n10. Troubleshooting and Debugging – A Systematic Playbook\n(See: Debugging Docker )\nYou will learn:\n\n\nMindset: containers as Linux processes\n\nStart with basics: is it running, using CPU, memory, and correct ports.\nAvoid jumping straight to complex tools.\n\n\n\nWhen a container will not start\n\nChecking logs vs exit codes.\nCommon errors: command not found, permission denied, missing environment variables.\n\n\n\nContainer runs, but the app is unreachable\n\nChecklist: internal port, published port, host firewall, binding to 0.0.0.0 vs 127.0.0.1.\nExample flow to validate each assumption.\n\n\n\nApp cannot reach another container\n\nVerifying shared networks and DNS resolution.\nUsing exec, curl, and ping from inside containers.\n\n\n\nPerformance and resource issues\n\nUsing docker stats and tools like top inside containers.\nUnderstanding CPU throttling and OOM kills.\n\n\n\nBuilding your own debug checklist\n\nTurning these steps into a reusable SOP.\nHow this mindset maps naturally to debugging Kubernetes Pods.\n\n\n\n\n11. Orchestration Hooks – How Docker Knowledge Transfers to Swarm and Kubernetes\n(See: From Docker to Orchestrators – How Your Mental Models Carry Over )\nYou will learn:\n\n\nWhy Docker alone is not enough in production\n\nRequirements like scheduling, self‑healing, scaling, and service discovery.\nWhere Docker Swarm and Kubernetes fit into the picture.\n\n\n\nConcept mapping table\n\nDocker container → Kubernetes Pod.\nDocker network → Kubernetes Service / CNI.\nDocker volume → PersistentVolume / PersistentVolumeClaim.\nCompose file → Kubernetes Deployments, Services, and other manifests.\n\n\n\nSwarm in one page (optional)\n\nBasic workflow: swarm init, creating services, scaling up and down.\nWhen Swarm is still useful for learning or small clusters.\n\n\n\nKubernetes‑oriented view\n\nWhy images, tags, and security hardening matter more under an orchestrator.\nHow Dockerfile and image choices affect rolling updates and autoscaling.\n\n\n\nPractice path: from Compose to Kubernetes\n\nTaking a Compose stack and gradually porting it to Kubernetes YAML.\nFocusing on building intuition instead of memorizing raw YAML.\n\n\n"},"Docker/1-Docker-Images":{"slug":"Docker/1-Docker-Images","filePath":"Docker/1 Docker Images.md","title":"1. Docker Images","links":[],"tags":[],"content":"If containers are lightweight processes, images are the immutable blueprints\nthat define what those processes look like at runtime.\nThis article walks from mental models to real Dockerfile behavior.\n1. Mental Model: Image vs Container vs Registry\nBefore touching commands, fix this mental picture in your head:\n\nImage\n\nA read‑only, versioned filesystem template plus metadata.\nThink: “frozen snapshot of a root filesystem + config”.\n\n\nContainer\n\nA running (or stopped) Linux process that uses an image as its root filesystem, plus a small writable layer on top.\nThink: “image + runtime state”.\n\n\nRegistry\n\nA remote storage for images, similar to a Git server for code.\nDocker Hub, ECR, GCR, GitHub Container Registry, etc.\n\n\n\nWorkflow in one sentence:\nYou build an image locally → tag it → push it to a registry → pull and run it on other machines (or clusters).\n\n2. Image Internals: Layers and the Build Graph\nDocker images are not single files; they’re stacks of layers, usually implemented via a union filesystem. Each layer:\n\nRepresents a change to the filesystem (add files, remove files, modify files).\nIs identified by a content hash.\nIs immutable once created.\nCan be shared between images to save space and speed up pulls.\n\nWhen you write a Dockerfile:\n\nFROM eclipse-temurin:21-jre-alpine\n\nWORKDIR /app\nCOPY app.jar /app/app.jar\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\n\n\nConceptually, Docker does something like this:\n\nStart from the base image eclipse-temurin:21-jre-alpine (many layers already).\nAdd a layer that creates/sets WORKDIR /app.\nAdd a layer that copies app.jar.\nAdd metadata for the CMD.\n\nEach instruction produces a new layer if it changes the filesystem. That layering is exactly what powers Docker’s build cache and efficient distribution.\n\n3. Build Cache: Why Instruction Order Matters\nThe Docker build cache works from top to bottom of your Dockerfile:\n\nFor each instruction, Docker checks: “Have I seen this same instruction with the same inputs before?”\nIf yes, it can reuse the previously built layer instead of rebuilding it.\nIf no (e.g., different files, different command), it must rebuild from that point downward.\n\nThat means:\n\nIf you put COPY . . near the top, any change in your source tree invalidates cache for all later steps, including heavy dependency installs.\nIf you separate dependency steps from source code, you can avoid re‑downloading dependencies on every small code change.\n\nExample: Java Maven app.\nSuboptimal Dockerfile:\nFROM eclipse-temurin:21-jdk-alpine\n\nWORKDIR /app\nCOPY . .\nRUN mvn -q -B package -DskipTests\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;target/app.jar&quot;]\n\n\nEvery time you change any file in the repo, COPY . . changes → cache invalidation from there down → Maven redownloads stuff and rebuilds.\nBetter, cache‑friendly multi‑stage Dockerfile:\nFROM eclipse-temurin:21-jdk-alpine AS build\nWORKDIR /app\n\n# 1. Copy only dependency descriptors and warm cache\nCOPY pom.xml .\nRUN mvn -q -B dependency:go-offline\n\n# 2. Copy source code and build\nCOPY src ./src\nRUN mvn -q -B package -DskipTests\n\n# 3. Runtime image\nFROM eclipse-temurin:21-jre-alpine\nWORKDIR /app\nCOPY --from=build /app/target/app.jar app.jar\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\n\n\nNow:\n\nChanging code in src does not invalidate the dependency cache step.\nOnly the final build step re‑runs, making rebuilds much faster.\n\nThis “dependency layer before source layer” pattern is universal: Node, Python, Go, Java, etc.\n\n4. Core Commands: List, Build, Tag, Push, Inspect\n4.1 Listing Images\nSee what’s on your machine:\ndocker images\n# or\ndocker image ls\n\n\nYou’ll see columns like:\n\nREPOSITORY (myorg/payment-service)\nTAG (1.0.0, latest)\nIMAGE ID\nCREATED\nSIZE\n\nGood hygiene: periodically scan this list and prune unused images.\n\n4.2 Building Images\nBasic build:\ndocker build -t myorg/payment-service:1.0.0 .\n\nKey points:\n\n. is the build context: everything under this directory is sent to the Docker daemon.\nAvoid sending huge directories (node_modules, target, .git) unless needed → use .dockerignore.\n\nDisable cache when you really want fresh layers:\ndocker build --no-cache -t myorg/payment-service:1.0.0 .\n\nYou’ll rarely want --no-cache in normal dev; it’s mainly for debugging or when the cache gets confusing.\n\n4.3 Tagging Images\nTags are just labels pointing to a specific image ID.\nCommon patterns:\n# Changing tag locally\ndocker tag myorg/payment-service:1.0.0 myorg/payment-service:latest\ndocker tag myorg/payment-service:1.0.0 myorg/payment-service:1.0.0-prod\n\n\nMentally treat tags like Git branches pointing to commits:\n\nlatest is not special; it’s just a tag.\nYou decide what latest means (usually “most stable release” or “most recent build”).\n\n\n4.4 Pushing &amp; Pulling (Registries)\nOnce tagged correctly, push to a registry:\n# Login once (if required)\ndocker login my-registry.example.com\n\n# Tag for registry namespace\ndocker tag myorg/payment-service:1.0.0 \\\n  my-registry.example.com/myorg/payment-service:1.0.0\n\n# Push\ndocker push my-registry.example.com/myorg/payment-service:1.0.0\n\n\nOn another machine (or your CI/CD):\ndocker pull my-registry.example.com/myorg/payment-service:1.0.0\ndocker run -d -p 8080:8080 my-registry.example.com/myorg/payment-service:1.0.0\n\n\nYou’ve now decoupled build (anywhere) from run (anywhere else).\n\n4.5 Inspecting Images and Their Layers\nTo see the full metadata:\ndocker inspect myorg/payment-service:1.0.0\n\nUseful sections:\n\nConfig.Env → default environment variables baked into the image.\nConfig.Cmd &amp; Config.Entrypoint → what runs by default.\nRootFS.Layers → the list of layer digests.\n\nTo see Dockerfile history:\ndocker history myorg/payment-service:1.0.0\n\nThis shows:\n\nEach layer’s size.\nThe instruction that produced it (when available).\nWhich layers are huge and could be optimized.\n\nYou can often spot mistakes like:\n\nGiant RUN layer that includes package caches.\nAccidentally copying the entire repo (COPY . .) when only a few directories are needed.\n\n\n5. Image Size: Why It Matters and How to Shrink It\nBig images hurt you in several ways:\n\nSlower pushes and pulls (more network usage).\nSlower deployments in Kubernetes clusters.\nMore disk usage on every node.\nLarger attack surface: more packages → more CVEs.\n\n5.1 Choose the Right Base Image\nFor example:\n\nubuntu or debian → full distribution, useful for tooling but heavy.\nalpine → very small, musl‑based, good for many apps but not all (e.g., some JVM or glibc‑dependent tools need tweaks).\nLanguage‑specific slim variants (python:3.13-slim, openjdk:21-jre-slim).\n\nIf your app is a Spring Boot service:\n\nA typical progression: openjdk:21-jre → eclipse-temurin:21-jre-alpine → maybe even distroless Java images later.\n\n5.2 Clean Up After Package Installs\nIn single RUN instructions, chain commands so you can remove caches in the same layer:\nRUN apk add --no-cache curl\n\n\nFor apt‑based images:\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends curl &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\n\nIf you don’t clean up, the package index stays in the layer and bloats your image.\n5.3 Multi-Stage Builds to Strip Tools\nAs shown earlier, multi‑stage builds keep compilers and build tools in a separate stage, and copy only artifacts to the final runtime image.\nFor example, Node:\nFROM node:22-alpine AS build\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci\nCOPY . .\nRUN npm run build\n\nFROM nginx:1.25-alpine\nCOPY --from=build /app/dist /usr/share/nginx/html\nEXPOSE 80\nCMD [&quot;nginx&quot;,&quot;-g&quot;,&quot;daemon off;&quot;]\n\n\nHere:\n\nNo Node, npm, or dev dependencies remain in the final image.\nFinal image is just Nginx + static files.\n\n\n6. Tagging Strategy: Knowing What’s in Prod\nBad pattern: everyone just uses :latest everywhere, and no one knows what commit is actually running in production.\nBetter approach:\n\nAlways tag images with:\n\nSemantic version: 1.0.0.\nBuild identifier: 1.0.0-20260130.1.\nGit SHA: app:git-abc1234 (even if only for internal use).\n\n\n\nExample flow for a CI build:\n\n\nBuild the image from commit abc1234.\n\n\nTag:\n\nmyorg/payment-service:1.0.0\nmyorg/payment-service:1.0.0-abc1234\nmyorg/payment-service:git-abc1234\n\n\n\nPush all tags.\n\n\nDeploy a specific tag (1.0.0-abc1234) in staging.\n\n\nPromote the same tag to production (retag or reuse directly) instead of rebuilding.\n\n\nThis makes it easy to answer “what exact code is running in prod?” and to roll back by deploying a previously known tag.\n\n7. Cleanup and Disk Management\nOver time, your Docker host accumulates:\n\nOld images.\nDangling images (no tags pointing to them).\nBuild cache for images you don’t use anymore.\n\nCommands you’ll use to stay sane:\n# Remove unused images (no containers use them)\ndocker image prune\n\n# More aggressive: remove all images not referenced by any container\ndocker image prune -a\n\n# Remove unused containers, networks, images (and optionally volumes if you add flags)\ndocker system prune\ndocker system prune -a\n\n\nUse aggressive flags (-a) with care, especially on shared or production systems.\nIn a dev environment, a periodic docker system prune -a is fine as long as you know you’ll be re‑pulling images.\n\n8. Security Basics for Images\nEven at the image level, you can make security better or worse.\nKey principles:\n\nMinimal base: fewer packages, fewer vulnerabilities.\nNo secrets baked into images:\n\nNever COPY .env or embed passwords as ENV variables in the Dockerfile.\n\n\nNon‑root where possible:\n\nCreate a dedicated user and USER switch to it in the Dockerfile.\n\n\n\nExample:\nFROM eclipse-temurin:21-jre-alpine\n\n# Create user and group\nRUN addgroup -S app &amp;&amp; adduser -S app -G app\n\nWORKDIR /app\nCOPY app.jar /app/app.jar\n\nUSER app\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\n\n\nThis won’t make you bulletproof, but it’s a baseline: an exploit in your app has fewer permissions inside the container.\n\n9. Putting It Together: A Typical Spring Boot Image\nHere’s a complete example that combines most of the ideas above.\n# Build stage\nFROM eclipse-temurin:21-jdk-alpine AS build\nWORKDIR /app\n\n# Dependencies\nCOPY pom.xml .\nRUN mvn -q -B dependency:go-offline\n\n# Source and build\nCOPY src ./src\nRUN mvn -q -B package -DskipTests\n\n# Runtime stage\nFROM eclipse-temurin:21-jre-alpine\n\n# Create non-root user\nRUN addgroup -S app &amp;&amp; adduser -S app -G app\n\nWORKDIR /app\nCOPY --from=build /app/target/app.jar app.jar\n\nUSER app\nEXPOSE 8080\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\n\n\nThis gives you:\n\nCache‑friendly builds.\nSmaller runtime image (no Maven/JDK).\nNon‑root user.\nExplicit port.\n\nFrom there:\ndocker build -t myorg/orders-api:1.0.0 .\ndocker tag myorg/orders-api:1.0.0 myorg/orders-api:latest\ndocker run -d --name orders-api -p 8080:8080 myorg/orders-api:1.0.0\n\n\nYou now have a well‑structured image lifecycle, instead of random copy‑paste Dockerfiles.\n"},"Docker/10-Debugging-Docker":{"slug":"Docker/10-Debugging-Docker","filePath":"Docker/10 Debugging Docker.md","title":"10. Debugging Docker","links":[],"tags":[],"content":"When something breaks in Docker, resist the urge to “try random commands until it works.” Containers are just Linux processes with extra plumbing, so you debug them the same way: check if they exist, if they run, what they log, what they listen on, and what they can reach.\nThis playbook gives you a sequence to follow so you don’t miss obvious issues.\n\n1. Mindset: treat containers like Linux processes\nBefore touching Docker‑specific tricks, anchor this:\n\nA container is a process (or a few) on the host, with:\n\nIts own filesystem view (from the image).\nIts own network namespace (its own IP/ports).\nResource limits via cgroups.\n\n\n\nSo, for any problem, ask in order:\n\nDoes the container exist? (docker ps -a)\nIs it running? (docker ps)\nWhat is its status and exit code? (docker inspect)\nWhat do the logs say? (docker logs)\nHow are CPU, memory, ports behaving? (docker stats, ss/netstat, top inside).\n\nDon’t jump straight to exotic tools. 80% of issues fall into:\n\nWrong command / missing binary.\nPermissions.\nWrong ports.\nNetwork miswiring.\nResource exhaustion.\n\n\n2. Container won’t start\nSymptom: you run docker run ..., it exits immediately, or docker compose up shows services flapping.\n2.1 Check status and exit code\nList recently exited container:\nbash\ndocker ps -a --latest\nInspect its state:\nbash\ndocker inspect &lt;container&gt; --format &#039;{{.State.Status}} {{.State.ExitCode}} {{.State.OOMKilled}} {{.State.Error}}&#039;\n\nStatus – e.g., exited.\nExitCode – 0 means “clean exit,” non‑zero indicates error.\nOOMKilled – true if the kernel killed it for memory.\n\n2.2 Look at logs\ndocker logs &lt;container&gt;\n# Or follow if it keeps flapping:\ndocker logs -f &lt;container&gt;\n\nCommon issues visible here:\n\nCommand not found:\n\nError like: executable file not found in $PATH.\nYour CMD/ENTRYPOINT points to a script/binary that doesn’t exist or isn’t executable.\n\n\nPermission denied:\n\nRunning a script without exec permission.\nWriting to a path the user can’t access.\n\n\nMissing environment/config:\n\nApplication fails early because required env vars or config files aren’t set.\n\n\n\n2.3 Reproduce interactively\nIf logs are unclear, start a container with an interactive shell using the same image:\ndocker run --rm -it &lt;image&gt; sh\n# or bash if available\n\nFrom there, manually run the command your Dockerfile uses as ENTRYPOINT/CMD:\njava -jar app.jar\n# or\n./start.sh\n\nYou’ll see errors in real time and can inspect the filesystem to understand what’s missing.\n\n3. Container runs, but app is unreachable\nSymptom: docker ps shows container as “Up”, but you can’t reach the app from host.\n3.1 Check internal port and binding\nFirst, exec into the container:\nbash\ndocker exec -it &lt;container&gt; sh\nInside:\n\nCheck if the app is listening on the expected port:\nbash\nnetstat -tulnp  # or ss -tulnp\nConfirm address:\n\nIf bound to 0.0.0.0:8080, it’s reachable from anywhere on that namespace.\nIf bound to 127.0.0.1:8080 inside the container, it’s reachable only from inside the container, which often breaks access through the Docker bridge.\n\n\n\nFix in app config:\n\nBind your server to 0.0.0.0 inside the container (not localhost).\n\n3.2 Check port publishing on the host\nOn the host:\nbash\ndocker ps\nLook at the PORTS column, e.g.:\n\n0.0.0.0:8080-&gt;8080/tcp – mapped correctly.\nEmpty or just 8080/tcp – no host port published.\n\nIf there’s no mapping, you probably forgot -p:\nbash\ndocker run -d --name api -p 8080:8080 myorg/api:1.0.0\n3.3 Host firewall and reachability\nIf mapping is correct but you still can’t reach:\n\n\nCurl from the host:\nbash\ncurl -v http://localhost:8080/health\n\n\nIf that fails, check:\n\nHost firewall rules (iptables, ufw, firewalld, cloud security groups).\nDocker Desktop/WSL routing on Windows/Mac if applicable.\n\n\n\nIf curl on host fails but curl inside container works:\n\nApp is healthy internally; issue is port mapping or firewall.\n\n\n\n\n4. App can’t reach another container (DB, cache, etc.)\nSymptom: API reports “can’t connect to DB/Redis/service” even though both containers are running.\n4.1 Verify same network\nInspect containers:\ndocker inspect api | grep -A3 Networks\ndocker inspect db  | grep -A3 Networks\n\nCheck they share a common network (e.g., app-net or Compose’s default).\nIf not:\n\nAttach them to the same user‑defined network:\n\ndocker network create app-net\ndocker network connect app-net api\ndocker network connect app-net db\n\n4.2 Check DNS name resolution\nInside the caller container (e.g., api):\ndocker exec -it api sh\nping db\n\n\nIf you get “unknown host”: the name db doesn’t resolve → wrong network or typo.\nOn user‑defined bridge networks, service/container names become DNS names automatically.\n\nIn Compose, the name used under services: is the DNS name (e.g., db).\n4.3 Curl/ping the dependency\nStill inside api:\napk add --no-cache curl  # if minimal image\ncurl -v http://db:5432   # or relevant port/protocol\n\nInterpretation:\n\nConnection refused:\n\nDB is not listening on the advertised port.\nDB is still starting; check DB logs.\n\n\nConnection timeout:\n\nNetwork misconfig, firewall, or wrong hostname.\n\n\nWorks, but app still fails:\n\nApp may be using wrong env vars or URL; verify its connection string.\n\n\n\n\n5. Performance and resource issues\nSymptom: container is “running” but slow, unresponsive, or periodically crashes.\n5.1 Check live resource usage\nOn the host:\ndocker stats\nSee:\n\nCPU% per container.\nMem usage / limit.\nNetwork I/O.\n\nIf one container is pegging CPU or hitting memory limits, that’s your suspect.\n5.2 OOM kills and exit code 137\nIf containers mysteriously exit with status 137, it usually means:\n\nProcess was killed by the OOM killer (out of memory).\nExit code 137 = 128 + 9 (SIGKILL) – typical for OOM conditions.\n\nCheck:\ndocker inspect &lt;container&gt; --format &#039;{{.State.ExitCode}} {{.State.OOMKilled}}&#039;\nIf OOMKilled=true:\n\nIncrease memory limit:\ndocker run -d --memory=&quot;1g&quot; ...\nOr reduce app memory usage (heap size, caches, concurrency).\n\n5.3 Debugging inside the container\nExec into the container and use familiar tools:\ndocker exec -it api sh\ntop            # or htop if installed\nps aux         # see processes\n\nCheck if:\n\nThere are too many threads/connectors.\nSome background process is hogging CPU.\nLogs show frequent GC or memory issues (for JVM apps).\n\n\n6. Building a personal “debug checklist”\nTo avoid flailing, encode this playbook as your SOP (Standard Operating Procedure). For any broken container:\n\n\nIs the container running?\n\ndocker ps -a → status and exit code.\nIf not running:\n\ndocker logs &lt;container&gt;\ndocker inspect &lt;container&gt; ...State.*\n\n\n\n\n\nIf running but not reachable from host:\n\nExec in: docker exec -it &lt;container&gt; sh\nCheck app listening and bind address (0.0.0.0 vs 127.0.0.1).\nCheck docker ps for correct PORTS mapping.\nTest with curl from host and inside.\n\n\n\nIf app can’t reach another container:\n\nConfirm same network (docker network inspect).\nUse service name as hostname (e.g., db).\nExec into caller; ping/curl to dependency.\n\n\n\nIf performance issues or crashes:\n\ndocker stats for CPU/memory.\nLook for OOMKilled, exit code 137.\nUse top, ps inside container to see noisy processes.\n\n\n\nIf still stuck:\n\nReproduce with a minimal image (e.g., alpine + curl) to isolate network/dns issues.\nSimplify Dockerfile/Compose config temporarily to narrow down the culprit.\n\n\n\nMapping to Kubernetes later\nIn Kubernetes, you’ll do the same steps, just with kubectl:\n\ndocker ps → kubectl get pods.\ndocker logs → kubectl logs.\ndocker exec → kubectl exec.\ndocker inspect → kubectl describe pod.\ndocker stats → metrics via kubectl top or monitoring stack.\n\nSo every debugging reflex you build with Docker translates almost 1:1 to Pods and Services in k8s, just with different commands."},"Docker/11-From-Docker-to-Orchestrators":{"slug":"Docker/11-From-Docker-to-Orchestrators","filePath":"Docker/11 From Docker to Orchestrators.md","title":"11. From Docker to Orchestrators","links":[],"tags":[],"content":"All the mental models you’ve built (images, containers, networks, volumes, Compose) are the foundation of Swarm and Kubernetes. Orchestrators don’t replace Docker; they automate it at cluster scale: scheduling, self‑healing, service discovery, and rolling upgrades.\n\n1. Why Docker alone isn’t enough in production\nDocker on a single host is great, but production needs more:\n\nScheduling\n\nDecide which node runs each container.\nRebalance when nodes join/leave or fail.\n\n\nSelf‑healing\n\nRestart crashed containers automatically.\nReschedule them to another node if the current node dies.\n\n\nScaling\n\nRun N replicas of a service across multiple nodes.\nScale up/down based on traffic or SLOs.\n\n\nService discovery &amp; load balancing\n\nGive clients a stable name (e.g., orders-api) even as containers move around.\nLoad balance across replicas.\n\n\n\nWhere orchestrators fit:\n\nDocker Swarm\n\nDocker’s built‑in clustering/orchestration mode.\nSimpler mental model, tight Docker integration.\n\n\nKubernetes\n\nDe‑facto standard orchestrator.\nRich API, strong ecosystem, steeper learning curve.\n\n\n\nYour Docker concepts (images, containers, networks, volumes, Compose) become the building blocks that Swarm and Kubernetes manage across many machines.\n\n2. Concept mapping table\nHere’s how your Docker knowledge maps into Kubernetes (and, conceptually, Swarm):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocker / Compose conceptKubernetes conceptMental mappingImageImage (same)Same artifact; pulled by nodes, defined in Pod specsContainerContainer inside a PodPod = 1+ tightly coupled containers (usually 1 for your apps)docker runPod (direct) / Deployment (managed Pods)Deployment continuously ensures Pods existCompose serviceDeployment + ServiceDeployment: replicas; Service: stable DNS + load balancingDocker networkCluster network + Service + CNICNI provides Pod IPs; Services provide stable virtual IPs/DNSContainer name DNSService name / Pod DNSdb in Compose → db Service in k8sDocker volumePersistentVolume (PV)Actual storage definitionVolume declaration in YAMLPersistentVolumeClaim (PVC)Pod requests storage via PVCBind mounthostPath volumeDirect host path mounting (used sparingly in k8s)Compose stack (file)Manifests: Deployment, Service, PVC, etc.Same intent, more detailed resourcesdocker logs / execkubectl logs / kubectl execSame debugging pattern, different CLI\nIf you’re comfortable with:\n\nImages and containers.\nNetworks and DNS service names.\nVolumes and data separation.\nCompose files as multi‑service specs.\n\n…then you already understand 70% of what Kubernetes objects are trying to express—just in a more explicit way.\n\n3. Swarm in one page (optional, but good for intuition)\nSwarm mode turns Docker into a simple orchestrator.\nBasic concepts\n\nNode – a Docker Engine participating in the swarm (manager or worker).\nService – a declarative description of a set of tasks (containers) with image, replicas, ports.\nTask – one running container which is part of a service.\nOverlay networks – multi‑host networks for inter‑service communication.\n\nCommon commands\nInitialize Swarm (on manager):\ndocker swarm init\nCreate a service:\ndocker service create \\\n  --name web \\\n  --replicas 3 \\\n  -p 80:80 \\\n  nginx:1.25-alpine\n\nScale:\ndocker service scale web=5\nInspect:\ndocker service ls\ndocker service ps web\n\nTakeaway:\n\nA Swarm service is to containers what docker-compose service is to single host—but with scheduling and replication across nodes.\nIt’s simpler than Kubernetes and uses the same Docker CLI mental model, so it’s good training for thinking about clusters.\n\n\n4. Kubernetes‑oriented view\nKubernetes is more verbose, but it’s still doing “Docker plus orchestration,” just with a very rich API.\nWhy images, tags, and security hardening matter more\nIn Kubernetes:\n\nYour images are pulled onto many nodes.\nAny mistake in the image (secrets baked in, running as root, huge size) is multiplied across the cluster.\nRolling updates, autoscaling, and chaos testing all assume images are:\n\nSmall (fast to pull).\nCorrectly tagged (so you know what’s running).\nReasonably secure (non‑root, minimal base).\n\n\n\nEverything you did right in Docker:\n\nMulti‑stage builds.\nNon‑root USER.\nStable tagging (1.2.3, git-abc1234).\nHEALTHCHECK endpoints.\n\n…makes Kubernetes deployments smoother:\n\nRolling updates can quickly pull new images.\nReadiness/liveness probes call your health endpoints.\nRBAC + PodSecurity settings play nicer with non‑root images.\n\nHow Dockerfile/image choices affect rolling updates and autoscaling\n\nRolling updates\n\nKubernetes pulls the new image on each node and starts new Pods gradually.\nLarge images → slow rollouts → longer partial deployments.\nMisconfigured health endpoints → Pods marked not ready → failed updates.\n\n\nAutoscaling\n\nHPA (Horizontal Pod Autoscaler) scales Pod count based on CPU, memory, or custom metrics.\nIf your image is CPU‑heavy due to debug tools or unbounded resource usage, autoscaling decisions are noisier.\nResource requests/limits must match realistic container behavior (the same --cpus, --memory you practiced in Docker).\n\n\n\nIn short: good Docker hygiene is a prerequisite for sane Kubernetes behavior.\n\n5. How to practice: from Compose to k8s\nThe best way to internalize the mapping is to port a real Compose stack to Kubernetes step by step.\nAssume you have a simple docker-compose.yml:\nversion: &quot;3.9&quot;\n\nservices:\n  db:\n    image: postgres:15-alpine\n    environment:\n      POSTGRES_DB: orders\n      POSTGRES_USER: orders_user\n      POSTGRES_PASSWORD: secret\n    volumes:\n      - pgdata:/var/lib/postgresql/data\n\n  api:\n    image: myorg/orders-api:1.0.0\n    ports:\n      - &quot;8080:8080&quot;\n    environment:\n      DB_HOST: db\n      DB_PORT: 5432\n      DB_NAME: orders\n      DB_USER: orders_user\n      DB_PASSWORD: secret\n    depends_on:\n      - db\n\nvolumes:\n  pgdata:\n\n\nStep‑wise migration outline\nStep 1 – API Deployment + Service\n\nCreate a Deployment for api:\n\nspec.template.spec.containers[0].image = myorg/orders-api:1.0.0.\nEnv vars copied into env: section.\n\n\nCreate a Service for api:\n\nType NodePort or LoadBalancer for external access to 8080.\nPort 8080 mapped to container port 8080.\n\n\n\nStep 2 – DB Deployment/StatefulSet + Service\n\nCreate a Deployment or StatefulSet for db:\n\nimage: postgres:15-alpine.\nEnv vars for DB credentials.\n\n\nCreate a PVC and Volume for data:\n\nPersistentVolumeClaim representing the pgdata volume.\nMount at /var/lib/postgresql/data.\n\n\nCreate a ClusterIP Service db:\n\nPort 5432.\nDNS name: db in the namespace.\n\n\n\nStep 3 – Wire API to DB\n\nIn the API Deployment, set:\n\nenv:\n  - name: DB_HOST\n    value: db\n  - name: DB_PORT\n    value: &quot;5432&quot;\n\n\nNow api Pods use the Service name db (just like Compose uses db as service name).\n\nStep 4 – Add health probes\n\nTranslate your Docker HEALTHCHECK to k8s probes:\n\nreadinessProbe:\n  httpGet:\n    path: /actuator/health\n    port: 8080\n  initialDelaySeconds: 10\n  periodSeconds: 10\n\nlivenessProbe:\n  httpGet:\n    path: /actuator/health\n    port: 8080\n  initialDelaySeconds: 30\n  periodSeconds: 30\n\n\nStep 5 – Apply resource limits\n\nUse your Docker experience with --cpus, --memory to choose:\n\nresources:\n  requests:\n    cpu: &quot;250m&quot;\n    memory: &quot;256Mi&quot;\n  limits:\n    cpu: &quot;1&quot;\n    memory: &quot;512Mi&quot;\n\n\nThis stepwise port keeps you focused on conceptual equivalence, not memorizing YAML.\n\nHow to build intuition, not just YAML muscle memory\n\nStart every k8s object by asking:\n\n“What is this in Docker/Compose terms?”\n“Which part of my Compose file is this mapping?”\n\n\nPractice debugging in k8s with the same mental sequence:\n\nkubectl get pods (like docker ps).\nkubectl logs (like docker logs).\nkubectl exec (like docker exec).\nkubectl describe (similar to docker inspect + events).\n\n\n\nYour Docker‑level understanding (images, containers, networking, volumes, Compose, debugging) is not wasted; it’s the core foundation that makes Kubernetes feel like a natural next layer instead of a completely foreign system."},"Docker/2-Docker-Containers":{"slug":"Docker/2-Docker-Containers","filePath":"Docker/2 Docker Containers.md","title":"2. Docker Containers","links":[],"tags":[],"content":"Most people start by thinking of containers as “lightweight VMs.” That mental model works for a few days, then hurts you for years. A container is just a process (or a few) with isolation and a custom filesystem view, built from an image. Once you see it that way, a lot of “Docker magic” becomes predictable.\nThis article walks through container internals, lifecycle, core commands, resource limits, and a practical troubleshooting mindset.\n\n1. Mental Model: What a Container Really Is\nUnder the hood, a container is:\n\nA Linux process (PID on the host)\nRunning with:\n\nIts own filesystem view (from an image + writable layer)\nIts own network namespace (own IP stack)\nIts own PID namespace (process tree inside container)\nResource limits enforced by cgroups\n\n\n\nKey idea:\nIf the container is “running,” that means there is a process running on the host. If the process exits, the container stops. There’s no “guest OS” like a VM.\nYou can prove this to yourself:\n# Run a simple container\ndocker run --name demo -d alpine:3.19 sleep 1000\n\n# On the host, find the PID\ndocker top demo\n# or\nps aux | grep sleep\n\n\nYou’ll see the sleep process; that’s your container.\n\n2. Container Lifecycle: From Image to Running Process\nFor a single container, the lifecycle looks like this:\n\nCreate\nDocker allocates metadata, filesystem, network namespace, etc.\n(You can do this explicitly with docker create, but docker run does it implicitly.)\nStart\nDocker launches the container’s main process (the ENTRYPOINT+CMD from the image).\nRunning\nThe process is alive. Docker tracks its stdout/stderr, resources, and exit code.\nStopped\nThe process exits. The container object + writable layer are still present on disk.\nRemoved\nDocker deletes the container’s metadata and filesystem layer.\n\nThe main command you use:\ndocker run\n\nis a convenience wrapper for:\n\ndocker create\ndocker start\n\nUnderstanding that helps when you debug:\n\nA container that “exits immediately” is just a process that finishes quickly.\nTo keep it running, you need a long‑running process (server, tail, sleep, etc.).\n\n\n3. Creating and Running Containers: Core Patterns\n3.1 One‑off interactive containers\nUse these whenever you want a temporary shell:\nbash\ndocker run --rm -it alpine:3.19 sh\nFlags breakdown:\n\n--rm → delete container when it stops.\n-it → interactive TTY (so you get a shell).\n\nMental model: this is your “disposable debugger” or “scratch VM,” but it’s still just a process.\n\n3.2 Long‑running services\nTypical pattern for APIs, web servers, etc.:\nbash\ndocker run -d --name web -p 8080:80 nginx:1.25-alpine\n\n-d → detached mode (run in background).\n--name web → stable name for logs, exec, etc.\n-p 8080:80 → map host port 8080 to container port 80.\n\nThis container:\n\nIs backed by a main process (nginx master process).\nWill stop if that process crashes or exits.\n\nIf it keeps dying, don’t think “VM crashed”; think “process crashed” and check logs.\n\n3.3 Environment variables and configuration\nPass configuration at runtime:\ndocker run -d --name orders-api \\\n  -p 8080:8080 \\\n  -e SPRING_PROFILES_ACTIVE=prod \\\n  -e DB_HOST=db \\\n  myorg/orders-api:1.0.0\n\n\nThis becomes:\n\nEnvironment variables visible to the process inside the container.\nExactly like export on a normal Linux host.\n\nThis is usually the right place for non‑secret configuration (URLs, flags, modes). Secrets should be handled more carefully in real systems (secret managers, etc.).\n\n4. Managing Containers Day‑to‑Day\nThink of these as your daily driver commands.\n4.1 Listing containers\ndocker ps        # running only\ndocker ps -a     # all containers (running + stopped)\n\n\nUseful columns:\n\nNAMES → what to use with logs, exec, inspect.\nSTATUS → “Up 5 minutes”, “Exited (0) 2 seconds ago”.\n\n4.2 Stopping, starting, removing\ndocker stop web     # send SIGTERM, wait, then SIGKILL after timeout\ndocker start web    # restart a stopped container\ndocker rm web       # remove container (must be stopped)\n\nIf you want to kill and recreate:\ndocker rm -f web    # force remove (stop + rm)\n\nTypical dev loop:\n\ndocker rm -f web\ndocker build -t myorg/web:dev .\ndocker run ...\n\n\n4.3 Logging: stdout and stderr as your primary log sink\nDocker automatically captures the main process’s stdout and stderr:\ndocker logs web          # historical logs\ndocker logs -f web       # follow logs (tail -f)\n\nGood practice:\n\nYour app should log to stdout/stderr (not to local files inside the container).\nThat way, orchestrators (Compose, Swarm, Kubernetes, log collectors) can pick up logs easily.\n\nFor a Spring Boot service:\n\nConfigure logs to go to console → Docker (and later Kubernetes) can aggregate them.\n\n\n4.4 Exec into containers\nWhen something is weird, “enter” the container:\ndocker exec -it web sh\n# or for Debian/Ubuntu based images:\ndocker exec -it web bash\n\n\nUse this to:\n\nInspect filesystem.\nRun curl, ping, or app‑specific debug commands.\nQuickly check config files and environment vars (env).\n\nIt’s equivalent to SSHing into a VM, but you’re really just attaching to a process’s namespace.\n\n5. Resources: Making Sure Containers Don’t Eat the Host\nBecause containers share the host kernel, they can starve each other if you don’t set limits.\n5.1 CPU limits\nbash\ndocker run -d --name cpu-demo --cpus=&quot;1.0&quot; myorg/task:1.0.0\nThis roughly constrains the container to 1 CPU core worth of time. Without limits, one container can saturate the host CPU, especially on dev machines.\n5.2 Memory limits\nbash\ndocker run -d --name mem-demo --memory=&quot;512m&quot; myorg/task:1.0.0\n\nIf the process allocates more than that, the kernel may kill it with OOM (Out Of Memory).\nYou’ll see exit code 137 (killed) or similar in Docker.\n\nCombine:\ndocker run -d --name api \\\n  -p 8080:8080 \\\n  --cpus=&quot;1&quot; \\\n  --memory=&quot;512m&quot; \\\n  myorg/orders-api:1.0.0\n\n\nThis is closer to how you’d run things in production.\n5.3 Checking usage: docker stats\ndocker stats\nGives live CPU, memory, network, I/O usage per container.\nIf one service is misbehaving, this is your quick “top” for containers.\n\n6. Restart Policies: Making Containers Survive Crashes\nIn pure Docker (without an orchestrator), restart policies give minimal self‑healing:\ndocker run -d --name api \\\n  --restart=on-failure \\\n  -p 8080:8080 \\\n  myorg/orders-api:1.0.0\n\n\nCommon policies:\n\nno (default): never restart automatically.\non-failure: restart only if exit code ≠ 0.\nalways: always restart if stopped.\nunless-stopped: restart unless you manually stopped it.\n\nUse cases:\n\non-failure for tasks that might crash but shouldn’t be resurrected if cleanly completed.\nalways / unless-stopped for long‑running services on standalone hosts.\n\nLater, in Kubernetes, this concept maps to Pod restart behavior controlled by the controller (Deployment, etc.).\n\n7. Containers vs Images vs Volumes: How Changes Persist\nA common confusion: “I edited a file inside the container, but when I recreate it, my changes are gone.”\nKey rules:\n\nImages are immutable.\nContainer writable layer is ephemeral:\n\nIf you docker rm the container, changes in that layer vanish.\n\n\nVolumes are persistent:\n\nThey outlive containers and can be attached to new ones.\n\n\n\nWorkflow implication:\n\nTo change the app code or binaries, you usually:\n\nChange source.\nRebuild image.\nStart new container from new image.\n\n\nTo persist data (database, uploads):\n\nUse volumes, not the container’s writable layer.\n\n\n\n\n8. Debugging Containers: A Practical Playbook\nWhen something “doesn’t work,” follow a steady sequence.\n8.1 Container exits immediately\n\nCheck status:\nbash\ndocker ps -a\nInspect exit code and logs:\nbash\ndocker logs my-container\n\nCommon causes:\n\nWrong command in CMD / ENTRYPOINT (executable not found).\nMain process completes and exits (e.g., script finishing).\nCrash due to missing config/env.\n\nFix: make sure the main process is long‑running and configured correctly.\n\n8.2 Container running, but port not accessible\nChecklist:\n\n\nIs the container running?\nbash\ndocker ps\n\n\nIs the app actually listening on the right port inside the container?\ndocker exec -it api sh\n# inside container:\nnetstat -tulnp  # or ss -tulnp\nMany apps bind to 127.0.0.1; inside a container, that’s still the container only.\nYou usually want to bind to 0.0.0.0.\n\n\nIs the port mapped on the host?\ndocker ps # look at PORTS column, e.g. 0.0.0.0:8080-&gt;8080/tcp\n\n\nCan you curl from the host?\ncurl http://localhost:8080/health\n\n\nIf it works inside the container but not outside:\n\nThe app might only listen on localhost inside the container.\nOr the port mapping (-p) is wrong/missing.\n\n\n8.3 Container can’t reach another container (DB, cache, etc.)\nChecklist:\n\nAre they on the same Docker network?\nIs the dependency container running?\nIs your app using the container name as hostname (on user‑defined networks)?\n\nDebug:\ndocker exec -it api sh\n# inside api container:\nping db\napk add --no-cache curl\ncurl http://db:5432  # or appropriate protocol/port\n\n\nIf DNS name doesn’t resolve, check that both are attached to the same user‑defined network and not using the default bridge incorrectly.\n\n9. Containers in the Bigger Picture: Why This Mental Model Matters\nOnce you internalize:\n\nContainer = process with isolation.\nImage = filesystem + metadata.\nVolume = persistent data.\n\nthen:\n\nDebugging Docker is just debugging Linux processes with extra tooling.\nMoving to Kubernetes is easier because Pods are also just wrapper abstractions around containers/processes.\nYou stop expecting “VM‑like” behaviors (like “I changed a file and it should persist forever”) and design images + volumes properly.\n"},"Docker/3-Docker-Networking":{"slug":"Docker/3-Docker-Networking","filePath":"Docker/3 Docker Networking.md","title":"3. Docker Networking","links":[],"tags":[],"content":"Docker networking feels magical until something doesn’t connect—then it feels like black magic. The cure is a clear mental model: each container has its own network stack, and Docker wires these stacks together using Linux primitives (bridges, veth pairs, iptables, etc.). Once you understand that, service‑to‑service communication becomes predictable instead of trial‑and‑error.\nThis article builds that model, then walks through real commands and a practical debugging playbook.\n\n1. Mental Model: What Happens When You docker run -p 8080:80 nginx\nLet’s start with the most common confusion: why does localhost sometimes work and sometimes not?\nWhen you run:\ndocker run -d --name web -p 8080:80 nginx:1.25-alpine\nDocker does roughly this under the hood:\n\nCreates a network namespace for the container (its own IP stack).\nCreates a veth pair: one end inside the container, one end on the host bridge.\nConnects the host end to a Linux bridge (often docker0) that acts like a virtual switch.\nAssigns the container an IP on that bridge (e.g. 172.17.0.2).\nSets up NAT rules so traffic hitting host :8080 gets DNAT‑ed to container 172.17.0.2:80.\n\nKey consequences:\n\nInside the container, localhost means “this container,” not your host.\nFrom the host, you normally reach containers via published ports (-p) or the container IP (bridge network).\nContainers talk to each other directly using their IPs or (on user‑defined networks) their service names.\n\n\n2. Built‑in Network Drivers: bridge, host, none, overlay\nDocker ships with several network “drivers.” You rarely need all of them, but knowing what they do matters.\n2.1 bridge – the default\n\nDefault for containers when you don’t specify --network.\nSingle‑host, NAT‑based networking.\nContainers get IPs like 172.17.x.x and talk to each other using those IPs.\n\nFor quick experiments, this is fine, but for anything non‑trivial you’ll prefer user‑defined bridges (still bridge driver, but created by you).\n2.2 User‑defined bridge – the recommended default\ndocker network create app-net\nProperties:\n\nContainers attached to this network get:\n\nTheir own IP in that network.\nBuilt‑in DNS: container names → IPs.\n\n\nDocker does some extra isolation and better defaults than the implicit bridge.\n\nTypical workflow:\ndocker run -d --name db --network app-net \\\n  -e POSTGRES_PASSWORD=secret \\\n  postgres:15-alpine\n\ndocker run -d --name api --network app-net \\\n  -e DB_HOST=db \\\n  myorg/orders-api:1.0.0\n\n\nNow:\n\napi can reach db at hostname db:5432.\nYou don’t care what the actual IPs are (Docker DNS resolves names).\n\n2.3 host – share host network\ndocker run --net=host nginx:1.25-alpine\n\nContainer shares the host’s network namespace.\nNo port mapping needed (or possible): if Nginx listens on :80 in container, it’s listening on host :80.\n\nPros:\n\nSlightly lower overhead, easier for some tools (like sniffers, some monitoring agents).\n\nCons:\n\nNo isolation.\nPort conflicts with host processes.\nNot available on Docker Desktop in the same way for all OSes.\n\nUse it sparingly, mostly for special system‑level stuff.\n2.4 none – fully isolated\ndocker run --network none alpine:3.19\n\nContainer has no network connectivity.\nOnly useful for specialized isolation scenarios or when you explicitly don’t want any network.\n\n2.5 overlay – multi‑host networks\n\nUsed with Docker Swarm (or similar setups).\nCreates a logical network spanning multiple hosts, often via VXLAN.\nLets services running on different nodes talk as if on the same LAN.\n\nFor now, just remember: overlay is for clustering; you’re unlikely to need it in simple, single‑host dev setups.\n\n3. Ports: Inside vs Outside, and Why localhost Lies\nEvery networked app has two sides:\n\nInside the container: the port the process binds to.\nOutside (host or other machines): how you expose that port.\n\nExample:\ndocker run -d --name api -p 8080:8080 myorg/orders-api:1.0.0\n\nInside container: your app binds to 8080.\nOn host: Docker publishes host 0.0.0.0:8080 → container &lt;container-ip&gt;:8080.\n\nCommon traps:\n\nApp binds to 127.0.0.1 inside the container:\n\nIt’s reachable from inside the container, but not from the host through bridged networking.\nYou usually want it to bind to 0.0.0.0 inside the container.\n\n\nForgot -p entirely:\n\nContainer can still serve other containers on same network.\nBut host curl http://localhost:8080 fails, because nothing is listening on host 8080.\n\n\n\nQuick check:\ndocker ps # Look at PORTS column: should show something like 0.0.0.0:8080-&gt;8080/tcp\n\n4. Docker Networking in Practice: Multi‑Container Stack\nLet’s wire up a simple but realistic stack: api + db.\n4.1 Create a network\ndocker network create app-net\n4.2 Run the database\ndocker run -d --name db --network app-net \\\n  -e POSTGRES_PASSWORD=secret \\\n  -v pgdata:/var/lib/postgresql/data \\\n  postgres:15-alpine\n\nObservations:\n\ndb gets IP on app-net (say 172.18.0.2).\nDocker DNS will resolve db to that IP for containers on app-net.\n\n4.3 Run the API\ndocker run -d --name api --network app-net \\\n  -p 8080:8080 \\\n  -e DB_HOST=db \\\n  -e DB_PORT=5432 \\\n  myorg/orders-api:1.0.0\n\n\nInside the container, your Spring Boot app will connect to Postgres at host db, port 5432.\nYou can now:\n\nFrom host: curl http://localhost:8080/actuator/health\nFrom api container: ping db or nc -z db 5432\n\n\n5. Inspecting Networks: Seeing the Wiring\nTo see what’s really going on, inspect:\ndocker network inspect app-net\nYou’ll get:\n\nSubnet and gateway for the network (e.g. 172.18.0.0/16).\nList of connected containers and their IP addresses.\nDriver (bridge) and some config.\n\nThis is your source of truth when you’re debugging connectivity:\n\nIs api actually on app-net?\nIs db actually on app-net?\nAre there multiple networks with similar names?\n\n\n6. Debugging Connectivity: A Step‑By‑Step Playbook\nWhen “service A can’t talk to service B,” don’t guess. Follow a consistent sequence.\n6.1 Check containers and networks\n\n\nAre both containers running?\nbash\ndocker ps\n\n\nAre they on the same network?\nbash\n\n\ndocker inspect api | grep -A3 Networks\ndocker inspect db  | grep -A3 Networks\n\nIf they’re not on the same user‑defined network, Docker DNS won’t resolve names across them.\n\n6.2 Exec into the caller and test\nFrom the api container:\ndocker exec -it api sh\n\n# Inside api:\nping db           # should resolve and respond (if ping is installed)\napk add --no-cache curl\ncurl -v http://db:5432   # or use nc/telnet if appropriate\n\n\nInterpretation:\n\nIf ping db fails with “unknown host,” DNS is broken (wrong network, typo in name).\nIf DNS works but curl or nc fails:\n\ndb might not be listening on the expected port.\nFirewall or misconfig inside db container.\n\n\n\n\n6.3 Host ↔ container access\nIf host can’t reach container:\n\n\nConfirm port mapping:\n`\n\n\ndocker ps\n# Check PORTS: 0.0.0.0:8080-&gt;8080/tcp?\n\n`\n\n\nCurl from host:\ncurl -v http://localhost:8080/health\n\n\nIf that fails, curl from inside container:\ndocker exec -it api sh curl -v http://localhost:8080/health\n\n\n\nWorks inside but not from host → usually port mapping or firewall issue.\nDoesn’t even work inside → app not listening on expected port or binding to wrong interface.\n\n\n7. Host Networking: When and Why (Not) to Use It\n--net=host (or --network host) gives containers the host’s network stack.\nPros:\n\nNo NAT overhead.\nUseful for network tools that need direct host access (e.g., sniffers, some monitoring agents).\nSimplifies some local dev scenarios (no -p needed).\n\nCons:\n\nNo port isolation; a container can bind host ports directly and conflict with host services.\nLess separation in terms of firewalling and security.\n\nExample:\ndocker run --net=host --name monitor \\   some/network-monitoring-tool\nFor most application containers, especially in multi‑tenant or production settings, stick with bridged networks and explicit -p mappings.\n\n8. From Docker Networking to Kubernetes Services\nYou’re ultimately heading to Kubernetes, so it’s useful to see how this mental model transfers:\n\nDocker user‑defined network → Kubernetes cluster network (via CNI plugin).\nDocker container name DNS → Kubernetes Service name and Pod DNS.\nDocker -p host:container → Kubernetes Service type NodePort / LoadBalancer / Ingress.\n\nIf you’re comfortable with:\n\nCreating networks,\nAttaching containers,\nUsing DNS names instead of IPs,\nDebugging using exec, curl, and network inspect,\n\nthen Kubernetes networking will feel like a structured extension, not a totally new universe.\n\n9. Practical Rules of Thumb\nTo anchor all this, here are some simple rules you can treat as defaults:\n\nUse user‑defined bridge networks for any multi‑container app, not the default bridge.\nAlways think: “inside vs outside” ports; don’t trust localhost without context.\nPrefer DNS names (container names) over hardcoded IPs for container‑to‑container calls.\nBuild a routine for debugging:\n\ndocker ps\ndocker network ls\ndocker network inspect\ndocker logs\ndocker exec + curl/ping\n\n\n\nThis turns “Docker networking is magic” into “Docker networking is just Linux networking with nicer defaults.”\n"},"Docker/4-Docker-Volumes":{"slug":"Docker/4-Docker-Volumes","filePath":"Docker/4 Docker Volumes.md","title":"4. Docker Volumes","links":[],"tags":[],"content":"Containers are designed to be disposable. That’s perfect for stateless services, but terrible for things like databases, uploads, and logs. Volumes are how Docker decouples the lifecycle of your data from the lifecycle of your containers.\nThis article builds a clear mental model of volumes, compares them with bind mounts, and gives you practical patterns and a debugging playbook.\n\n1. Mental Model: Container FS vs Volume\nInside every container you have:\n\nA read‑only image filesystem (from the image layers).\nA writable layer on top (container’s own changes).\n\nWhen you docker rm a container, that writable layer (and all its changes) disappears. That’s why:\n\nWriting a Postgres data directory to /var/lib/postgresql/data inside the container without a volume means your database disappears when the container is deleted.\nEditing app code inside the container is a temporary hack; it vanishes on rebuild.\n\nVolumes solve this by creating separate, managed storage that:\n\nLives outside the container’s writable layer.\nCan be attached to any container.\nSurvives container deletion.\n\nSo think:\n\nContainer filesystem = ephemeral.\nVolume = persistent, container‑independent data.\n\n\n2. Types of Storage: Named Volumes vs Bind Mounts vs Tmpfs\nDocker supports three main storage concepts:\n\n\nNamed volumes\n\nManaged by Docker.\nIdentified by name (pgdata, redis-data).\nStored under Docker’s data directory (path differs by OS).\nBest choice for persistent data in most cases.\n\n\n\nBind mounts\n\nDirect mapping of a host path into the container (/home/user/app:/app).\nGreat for development (live code reload), or when you explicitly want to use a host directory.\nYou manage the directory; Docker just mounts it.\n\n\n\nTmpfs mounts\n\nIn‑memory filesystem for ephemeral data.\nContents are never written to disk on the host.\nUseful for sensitive or high‑churn temporary data.\n\n\n\nHigh‑level rule:\n\nUse named volumes for app data (databases, queues).\nUse bind mounts for development workflows and special host integrations.\nUse tmpfs for sensitive temp data or when you really want in‑memory only.\n\n\n3. Named Volumes: Your Default for Persistent Data\n3.1 Creating and listing named volumes\ndocker volume create pgdata\ndocker volume ls\ndocker volume inspect pgdata\n\n\nInspect shows:\n\nName.\nDriver (usually local).\nMountpoint on the host (where Docker stores the data).\n\nYou typically don’t touch that path directly; you let Docker manage it.\n3.2 Using a named volume with a database\nPostgres example:\ndocker run -d --name db \\\n  -e POSTGRES_PASSWORD=secret \\\n  -v pgdata:/var/lib/postgresql/data \\\n  postgres:15-alpine\n\n\nHere:\n\npgdata is the named volume.\nInside the container, Postgres writes to /var/lib/postgresql/data.\nOn the host, Docker maps that to some internal path for pgdata.\n\nNow try:\n\n\nInsert some data into the DB.\n\n\nDestroy the container:\nbash\ndocker rm -f db\n\n\nStart a new container with the same volume:\n\n\ndocker run -d --name db2 \\\n  -e POSTGRES_PASSWORD=secret \\\n  -v pgdata:/var/lib/postgresql/data \\\n  postgres:15-alpine\n\nYour data is still there. The container is disposable; the volume is the durable component.\n3.3 Sharing volumes between containers\nYou can attach the same volume to multiple containers:\ndocker run -d --name db-admin \\\n  --network app-net \\\n  -v pgdata:/var/lib/postgresql/data:ro \\\n  some/pgadmin-image\n\n\n\n:ro makes it read‑only for that container.\nBoth DB and admin UI share the same underlying data directory.\n\nThis pattern is powerful but be careful:\n\nTwo writers to the same data directory → potential corruption unless the app explicitly supports it.\n\n\n4. Bind Mounts: Perfect for Development\nBind mounts map a host directory directly into the container.\n4.1 Basic example: mounting source code\ndocker run -d --name web-dev \\\n  -p 3000:3000 \\\n  -v &quot;$PWD/src:/usr/src/app&quot; \\\n  node:22-alpine \\\n  sh -c &quot;cd /usr/src/app &amp;&amp; npm install &amp;&amp; npm run dev&quot;\n\n\nHere:\n\nHost $PWD/src is mounted into container /usr/src/app.\nWhen you edit source files on the host, changes are instantly visible inside the container.\nPerfect for hot‑reloading dev servers (Node, React, Angular, etc.).\n\n4.2 Pros and cons of bind mounts\nPros:\n\nGreat developer experience (no need to rebuild images for every code change).\nYou can use your host editors and tools naturally.\n\nCons:\n\nBehavior differs by OS; Docker Desktop uses a VM and can have performance quirks.\nPermissions can be tricky (UID/GID mismatches).\nIn production, you often don’t want arbitrary host directories exposed to containers.\n\nRule of thumb:\n\nBind mounts: local dev, debugging, special host integration.\nNamed volumes: production‑oriented persistent data and portable stacks.\n\n\n5. Tmpfs Mounts: In‑Memory Storage for Sensitive or Ephemeral Data\nTmpfs mounts keep data in RAM, never on disk.\nExample:\ndocker run -d --name cache \\\n  --tmpfs /var/cache/app \\\n  myorg/cache-heavy-app:1.0.0\n\nUse cases:\n\nSensitive temporary data that you do not want persisted.\nHigh‑churn temporary caches where disk I/O would be a bottleneck.\n\nBut remember:\n\nData disappears when container stops.\nIt consumes RAM, so monitor memory usage.\n\n\n6. Managing Volumes Over Time\nVolumes can accumulate just like images. You want a basic hygiene routine.\n6.1 Listing and inspecting\ndocker volume ls\ndocker volume inspect some-volume\n\nFind:\n\nWhich volumes exist.\nWhere they live on disk.\nWhich containers are using them (via inspect/Docker metadata or by cross‑checking containers).\n\n6.2 Pruning unused volumes\ndocker volume prune\n# Add -f if you want to skip confirmation\n\nThis removes volumes that are not used by any container.\nBe careful:\n\nIf you have stopped containers that you think you might restart, removing volumes can destroy their data.\nIn dev environments, regular prune is fine if you treat data as disposable.\n\n\n7. Backups and Migration: Treat Volumes as Data Stores\nSince a volume is just a filesystem path on the host, you can back it up using standard tools. A common pattern is to run a temporary helper container that mounts the volume.\nFor example, to tar a Postgres data volume:\ndocker run --rm \\\n  -v pgdata:/data \\\n  -v &quot;$PWD:/backup&quot; \\\n  alpine:3.19 \\\n  sh -c &quot;cd /data &amp;&amp; tar czf /backup/pgdata-backup.tgz .&quot;\n\n\npgdata is mounted at /data inside the helper container.\nHost current directory is mounted at /backup.\nYou create an archive in the host current directory.\n\nTo restore, you’d reverse the process (carefully, ideally when DB is stopped).\nThis approach works for any volume‑backed data:\n\nMessage broker data.\nFile uploads.\nAnything that lives in a volume.\n\n\n8. Permission &amp; Ownership Gotchas\nContainers run processes as some user (often root by default, but ideally a non‑root user you create). When you mount volumes or bind mounts, file ownership matters.\nTypical problems:\n\nHost path is owned by user:group that doesn’t match container user.\nContainer user cannot read/write the mounted directory.\n\nPatterns to avoid pain:\n\n\nAlign UIDs/GIDs\n\nRun container with a specific UID that matches host directory owner.\nOr create the user in the Dockerfile with the right UID.\n\n\n\nInitialize volume from container\n\nLet the container create and own its data directories on first run.\nAvoid pre‑creating them on host with mismatched ownership.\n\n\n\nExample in Dockerfile:\nRUN addgroup -S app &amp;&amp; adduser -S app -G app\nUSER app\n\nThen ensure that the volume mount point inside the container is writable by app.\nIf you see permission denied errors on mounted paths, think: “Which user am I inside the container, and who owns this directory?”\n\n9. Volumes in Orchestrated Environments (Preview)\nIn basic Docker, volumes are relatively simple:\n\nNamed volume → some directory on the host.\nBind mount → exactly the host path you specify.\n\nIn Kubernetes and other orchestrators, these concepts grow into:\n\nPersistentVolumes (PV) and PersistentVolumeClaims (PVC).\nDynamic provisioning from storage classes (EBS, GCE PD, NFS, Ceph, etc.).\nVolume plugins for cloud, network storage, etc.\n\nYour Docker mental model still holds:\n\nPods mount volumes for persistence.\nContainers inside Pods see those volumes as directories, just like your Docker containers.\nThe orchestrator manages lifecycle, scheduling, and attachment across nodes.\n\nSo the core idea—data lives outside disposable containers—remains exactly the same.\n\n10. Practical Rules of Thumb for Volumes\nTo wrap this into actionable habits:\n\nNever rely on the container’s writable layer for important data; it dies with the container.\nUse named volumes for databases and anything you want to survive container recreation.\nUse bind mounts mainly for development or when you explicitly want host‑side control.\nRegularly inspect and prune unused volumes in dev environments.\nAlways think about ownership and permissions when mounting directories.\nPractice backup/restore with at least one real service (e.g., Postgres) so it’s muscle memory.\n"},"Docker/5-Dockerfile-Mastery":{"slug":"Docker/5-Dockerfile-Mastery","filePath":"Docker/5 Dockerfile Mastery.md","title":"5. Dockerfile Mastery","links":[],"tags":[],"content":"A Dockerfile is not a shell script; it’s a deterministic build recipe that produces an immutable image. Once you understand how instructions translate into layers and how the cache behaves, you stop copy‑pasting random snippets and start designing Dockerfiles like you design code: intentionally, with tradeoffs in mind.\nBelow we walk through the mental model and every core instruction, then finish by refactoring a bad Dockerfile into a production‑quality one.\n\n1. Mental model: Dockerfile as a deterministic build recipe\nTop‑down execution and build graph\nWhen you run docker build:\n\nDocker sends the build context (files from your directory, minus .dockerignore) to the daemon.\nIt parses the Dockerfile top to bottom.\nEach instruction (FROM, RUN, COPY, etc.) produces a new layer (except a few pure‑metadata ones like some LABELs).\nThe final image is a stack of these layers.\n\nYou can think of it as:\n\nFROM → base layers.\nEvery RUN/COPY/ADD → new layer on top.\nCMD/ENTRYPOINT/ENV/EXPOSE/USER/WORKDIR/HEALTHCHECK → configuration for how containers will run.\n\nBuild cache and layer invalidation\nDocker’s build cache works instruction by instruction:\n\nFor each instruction, the daemon checks: “Have I already built this instruction before with the same inputs?”\nInputs include:\n\nThe instruction text itself.\nThe files it touches from the build context (e.g., paths in COPY).\n\n\nIf identical, Docker reuses the old layer instead of re‑executing.\n\nIf an instruction changes, or the files it touches change, that instruction and all instructions after it must be rebuilt.\nThat means:\n\nIf you do COPY . . early, then any code change invalidates the cache for the rest of the file: expensive.\nIf you split dependency installation and source copy, you can reuse the dependency layer even when code changes.\n\nExample: better ordering for a Maven build:\n# 1. Copy pom.xml only → stable if deps don’t change\nCOPY pom.xml .\nRUN mvn -q -B dependency:go-offline\n\n# 2. Copy source → changes more often\nCOPY src ./src\nRUN mvn -q -B package -DskipTests\n\n\nThis is the core “art of small layers”: structure your Dockerfile so that rarely changing steps go first, frequently changing steps go later.\n\n2. Core instructions, with intent\nLet’s go instruction by instruction: what it really means and when to use it.\n2.1 FROM\nSets the base image (starting point) for your build.\nExamples:\nFROM eclipse-temurin:21-jre-alpine    # Java runtime\nFROM node:22-alpine                   # Node.js\nFROM python:3.13-slim                 # Python\n\nWhen to use:\n\nAlways at the top of each stage.\nFor multi‑stage builds, each new stage starts with FROM.\n\nDesign considerations:\n\nChoose minimal but appropriate base (slim/alpine/distroless vs full OS).\nPin versions (:21-jre-alpine, not :latest) for reproducibility.\nPrefer official or trusted vendor images.\n\n\n2.2 RUN\nExecutes a command at build time and commits the result as a new layer.\nExamples:\nRUN apk add --no-cache curl\n\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends curl &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\nWhen to use:\n\nInstall packages and dependencies.\nBuild/compile your application.\nPerform one‑time setup that affects filesystem contents.\n\nDesign tips:\n\nCombine related steps into a single RUN when they are tightly coupled, especially for package installation + cleanup.\nAvoid leaving caches behind in earlier layers (do update/install/cleanup in the same RUN).\nDon’t install heavy dev tools in the final runtime stage; keep them in build stages.\n\n\n2.3 COPY and ADD\nCopy files/directories into the image.\n\nCOPY:\n\nCOPY app.jar /app/app.jar\nCOPY src/ /app/src/\n\n\nADD (with extra capabilities):\n\nADD data.tar.gz /data/        # auto‑extracts\nADD example.com/file /tmp/file   # fetches from URL\n\nWhen to use:\n\nCOPY for almost everything: it’s explicit and predictable.\nADD only when you specifically need its special behaviors (tar auto‑extract or URL fetch).\n\nWe go deeper on COPY vs ADD in section 3.\n\n2.4 WORKDIR\nSets the working directory for subsequent instructions.\nWORKDIR /app\nCOPY app.jar .\nRUN ls -l\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\n\nWhen to use:\n\nTo avoid full absolute paths everywhere.\nTo make your Dockerfile cleaner and commands shorter.\n\nNotes:\n\nIf the directory doesn’t exist, Docker creates it.\nYou can set WORKDIR multiple times; each new one becomes the current working directory for the following instructions.\n\n\n2.5 ENV\nSets environment variables that will be available at build time and runtime.\nENV SPRING_PROFILES_ACTIVE=prod\nENV JAVA_OPTS=&quot;-Xms256m -Xmx512m&quot;\n\nWhen to use:\n\nTo bake default runtime configuration into the image.\nTo set environment variables needed during build (though ARG is often more appropriate for build‑only values).\n\nContainers can override ENV at runtime:\nbash\ndocker run -e SPRING_PROFILES_ACTIVE=stage myorg/app:1.0.0\n\n2.6 ARG\nDefines build‑time variables available only during build.\nARG BUILD_VERSION=dev\nARG GIT_SHA\n\nRUN echo &quot;Building version $BUILD_VERSION from $GIT_SHA&quot;\nLABEL version=&quot;$BUILD_VERSION&quot; git_sha=&quot;$GIT_SHA&quot;\n\nWhen to use:\n\nTo pass build metadata (versions, git SHA, build date).\nTo toggle build behavior (e.g., ARG ENABLE_DEBUG_TOOLS=true used only in build stage).\n\nThey do not exist in the resulting container environment unless you explicitly copy them into ENV or labels.\nBuild with:\ndocker build \\\n  --build-arg BUILD_VERSION=1.2.3 \\\n  --build-arg GIT_SHA=abc1234 \\\n  -t myorg/app:1.2.3 .\n\nWe explore ENV vs ARG patterns in section 4.\n\n2.7 EXPOSE\nDocuments which ports the container listens on.\nEXPOSE 8080\nEXPOSE 8080 8443\n\nWhen to use:\n\nAs a form of documentation and metadata for tools.\nSo docker run -P knows which ports to auto‑publish.\n\nImportant:\n\nEXPOSE does not actually open ports on the host; you still need -p in docker run.\n\n\n2.8 USER\nSets which user the following instructions (and the default container process) will run as.\nRUN addgroup -S app &amp;&amp; adduser -S app -G app\nUSER app\n\nWhen to use:\n\nTo drop root privileges and run your app as a non‑root user.\nAs part of basic container hardening.\n\nOnce you set USER, subsequent RUN instructions also run under that user unless you switch back.\nWe discuss security patterns more in section 6.\n\n2.9 CMD\nDefines the default command (and/or arguments) to run when a container starts.\nExec form (recommended):\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\nShell form:\nCMD java -jar app.jar\nWhen to use:\n\nTo set the default command or default arguments for your container.\nExpecting that users (or orchestrators) may override it at docker run time.\n\nCMD is often combined with ENTRYPOINT (see section 5).\n\n2.10 ENTRYPOINT\nDefines the fixed executable that will always be run for this container.\nENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\nWhen to use:\n\nWhen your container is fundamentally “this binary”.\nTo make sure the main process is always your app, and CLI args are treated as arguments to it.\n\nTogether with CMD, it becomes:\nENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\nCMD [&quot;--spring.profiles.active=prod&quot;]\n\nAt runtime, Docker effectively runs:\njava -jar app.jar --spring.profiles.active=prod\nWe detail CMD vs ENTRYPOINT in section 5.\n\n2.11 HEALTHCHECK\nDefines a command Docker runs periodically to verify container health.\nHEALTHCHECK --interval=30s --timeout=3s --retries=3 \\\n  CMD curl -f http://localhost:8080/actuator/health || exit 1\n\nWhen to use:\n\nWhen you want a signal beyond “process is running.”\nTo help orchestrators avoid routing traffic to unhealthy containers.\n\nGood healthchecks are cheap and local; they shouldn’t depend heavily on external systems unless that’s part of your SLA.\n\n3. COPY vs ADD – the subtle traps\nYou almost always want COPY. The reason is simple: ADD does more than just copy, and that extra behavior can surprise you.\nWhat ADD does in addition\n\nIf the source is a tar archive and the destination is a directory, ADD auto‑extracts it.\nIf the source is a URL, ADD fetches it over HTTP/HTTPS.\n\nExample:\nADD data.tar.gz /data/   # /data contains extracted contents\nThis may look convenient, but:\n\nIt hides logic: you can’t tell from the Dockerfile alone that extraction is happening.\nYou have less control over where and how it extracts.\nUsing URLs in ADD can complicate caching and reproducibility (network changes vs build cache).\n\nWhy COPY is preferred\n\nCOPY is dumb and predictable: “copy exactly these files to exactly there.”\nIt interacts clearly with .dockerignore.\nYou can handle downloads and extractions explicitly in a RUN, where you also control cleanup.\n\nExample with RUN instead of ADD URL:\nRUN curl -L example.com/file.tar.gz -o /tmp/file.tar.gz &amp;&amp; \\\n    tar xzf /tmp/file.tar.gz -C /data &amp;&amp; \\\n    rm /tmp/file.tar.gz\n\nYou control:\n\nWhere the file goes.\nHow it’s extracted.\nThat temporary artifact is cleaned up in the same layer.\n\nRule of thumb: default to COPY; reach for ADD only when you truly want its special behaviors and understand the tradeoffs.\n\n4. ENV and ARG patterns\nBuild‑time vs runtime configuration\n\nARG: build‑time only, not available in running containers unless explicitly propagated.\nENV: runtime environment variables baked into the image (and also available during build after they’re set).\n\nPattern: pass build metadata via ARG, then store it via LABEL or ENV if needed at runtime.\nExample:\nARG BUILD_VERSION=dev\nARG GIT_SHA=unknown\n\nLABEL version=&quot;$BUILD_VERSION&quot; git_sha=&quot;$GIT_SHA&quot;\n\nENV APP_VERSION=$BUILD_VERSION\n\nBuild:\ndocker build \\\n  --build-arg BUILD_VERSION=1.2.3 \\\n  --build-arg GIT_SHA=abc1234 \\\n  -t myorg/app:1.2.3 .\n\nNow:\n\nThe image has labels with version and SHA.\nThe container has APP_VERSION at runtime.\n\nRuntime profiles with ENV + override\nFor a Spring Boot app:\ntext\nENV SPRING_PROFILES_ACTIVE=prod\nYou can override per environment:\n# Staging\ndocker run -e SPRING_PROFILES_ACTIVE=stage myorg/app:1.2.3\n\n# Dev\ndocker run -e SPRING_PROFILES_ACTIVE=dev myorg/app:1.2.3\n\nDesign principle:\n\nDon’t rebuild images just to change environment‑specific settings.\nUse ENV for defaults; override via runtime env variables in Compose/Kubernetes.\n\n\n5. CMD vs ENTRYPOINT patterns\nFixed executable vs overridable parameters\nThink of:\n\nENTRYPOINT = binary to always run.\nCMD = default arguments, overridable at runtime.\n\nCanonical pattern:\nENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\nCMD [&quot;--spring.profiles.active=prod&quot;]\n\nThen:\n# uses default profile=prod\ndocker run myorg/app:1.0.0\n\n# override profile with CLI arg\ndocker run myorg/app:1.0.0 --spring.profiles.active=stage\n\n\nDocker effectively does:\njava -jar app.jar --spring.profiles.active=stage\nCommon mistakes\n\n\nPutting everything in CMD and then overriding the whole thing at runtime:\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;,&quot;--spring.profiles.active=prod&quot;]\nIf someone runs docker run myorg/app:1.0.0 my-custom-command, your app doesn’t run at all.\n\n\nUsing shell form and breaking signal handling:\nCMD java -jar app.jar\nPID 1 is /bin/sh -c, not your app; SIGTERM/SIGINT may not propagate cleanly.\n\n\nBetter: use exec form for both ENTRYPOINT and CMD.\n\n6. Security‑aware Dockerfile writing\nThe Dockerfile is your first line of defense. A few simple patterns dramatically reduce risk.\n6.1 Non‑root user\nCreate a dedicated user and drop root:\nFROM eclipse-temurin:21-jre-alpine\n\nRUN addgroup -S app &amp;&amp; adduser -S app -G app\n\nWORKDIR /app\nCOPY app.jar /app/app.jar\n\nUSER app\n\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\n\nBenefits:\n\nEven if your app is compromised, the attacker has fewer privileges in the container.\nLimits damage from misconfigurations.\n\nRemember to ensure directories your app writes to are writable by app.\n6.2 No secrets in images\nNever:\n\nCOPY .env /app/.env\nENV DB_PASSWORD=supersecret in a public or widely shared image.\n\nBetter:\n\nInject secrets at runtime via environment variables, secret managers, or orchestrator mechanisms (Kubernetes Secrets, etc.).\nRestrict secrets to runtime configuration, not immutable image configuration.\n\n6.3 Minimal surface area\n\nUse minimal base images (alpine, slim, distroless).\nAvoid unnecessary tools (curl, ping, etc.) in the final runtime image; keep them only in build or debug images.\n\nThe smaller and simpler your image, the fewer attack vectors and CVEs you’ll carry.\n\n7. Example: refactoring a bad Dockerfile\n7.1 Naïve Dockerfile\nFROM openjdk:21\n\nWORKDIR /app\nCOPY . .\nRUN mvn package\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;target/app.jar&quot;]\n\nIssues:\n\nHeavy base image (full OS + JDK).\nCopies entire context (including .git, target, docs, etc.).\nPoor cache usage: any code change invalidates Maven dependency downloads.\nRuns as root.\nNo explicit healthcheck or environment defaults.\nBuild tools (Maven, JDK) are in the same image used in production.\n\n7.2 Refactored, production‑grade Dockerfile\n# Stage 1: build\nFROM eclipse-temurin:21-jdk-alpine AS build\nWORKDIR /app\n\n# Install dependencies based on pom.xml (good cache usage)\nCOPY pom.xml .\nRUN mvn -q -B dependency:go-offline\n\n# Copy source and build\nCOPY src ./src\nRUN mvn -q -B package -DskipTests\n\n# Stage 2: runtime\nFROM eclipse-temurin:21-jre-alpine\n\n# Create non-root user\nRUN addgroup -S app &amp;&amp; adduser -S app -G app\n\nWORKDIR /app\nCOPY --from=build /app/target/app.jar app.jar\n\n# Runtime defaults\nENV SPRING_PROFILES_ACTIVE=prod\n\nUSER app\nEXPOSE 8080\n\n# Healthcheck (simple example)\nHEALTHCHECK --interval=30s --timeout=3s --retries=3 \\\n  CMD curl -f http://localhost:8080/actuator/health || exit 1\n\n# Binary + default args pattern\nENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\nCMD [&quot;--spring.profiles.active=prod&quot;]\n\nWhat improved:\n\nMulti‑stage build: JDK + Maven only in build stage; final image has JRE only.\nSmaller, more secure runtime image.\nBetter cache behavior: dependencies cached separately from source.\nNon‑root user.\nEXPOSE and HEALTHCHECK provide standard metadata and health signal.\nENTRYPOINT/CMD split allows overriding profile without rewriting command.\n\nThis is “Dockerfile mastery”: the same basic goal (run a Spring Boot JAR) implemented in a way that respects performance, security, maintainability, and observability."},"Docker/6-Multi‑Stage-Docker-Builds":{"slug":"Docker/6-Multi‑Stage-Docker-Builds","filePath":"Docker/6 Multi‑Stage Docker Builds.md","title":"6. Multi‑Stage Docker Builds","links":[],"tags":[],"content":"Multi‑stage builds exist to solve a very concrete problem: images that are huge, slow to ship, and full of compilers and dev tools that have no business being in production. You keep all the heavy build tooling in one stage, and ship only the small, clean runtime stage to prod.\n\n1. The problem: fat images with build tools inside\nIn a naïve Dockerfile, you often see something like:\nFROM node:22\n\nWORKDIR /app\nCOPY . .\nRUN npm install\nRUN npm run build\nCMD [&quot;npm&quot;,&quot;start&quot;]\n\nor for Java:\nFROM maven:3.9-eclipse-temurin-21\n\nWORKDIR /app\nCOPY . .\nRUN mvn -q -B package\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;target/app.jar&quot;]\n\n\nWhat’s wrong here?\n\nJDK, Maven, Node toolchain are all present in the final image.\nYou’re shipping:\n\nPackage managers (npm, apt, Maven).\nCompilers and build tools.\nDev dependencies (e.g., node_modules with test tooling).\n\n\nImpact:\n\nPull time: every node in your cluster pulls a big image.\nDisk usage: each node stores these large layers.\nSecurity: more binaries and libraries → more potential CVEs → more noise in vulnerability scans.\n\n\n\nThe app only needs the built artifact (JAR, binary, static assets), not the entire toolbox used to produce it.\n\n2. Concept: separate build stage from runtime stage\nMulti‑stage builds let you define multiple FROMs in a single Dockerfile. Each FROM starts a new “stage” with its own filesystem. You can then selectively copy build outputs from earlier stages into a minimal final stage.\nCore mechanics:\nFROM some-builder-image AS build\n# ... build commands ...\n\nFROM small-runtime-image\n# Copy artifacts from build stage\nCOPY --from=build /path/in/build-stage /path/in/runtime\n# ... runtime config ...\n\nKey elements:\n\nAS build names the stage build. You can choose any name (e.g., builder, compile).\nCOPY --from=build pulls files only from that stage’s filesystem into the current stage.\nEarlier stages don’t exist in the final image unless you copy data explicitly.\n\nWhy it’s “cheap”:\n\nDocker still layers everything efficiently.\nStages share cached layers like normal builds.\nOnly the last stage becomes the final image that’s pushed/pulled in normal workflows.\n\nSo you get:\n\nFull power of a heavy build environment.\nA clean, minimal runtime image used in production.\n\n\n3. Language‑specific examples\n3.1 Node: build static assets, serve with Nginx\nGoal: use Node only to build the frontend, then serve static files with Nginx.\n# Stage 1: build\nFROM node:22-alpine AS build\nWORKDIR /app\n\n# Install dependencies\nCOPY package*.json ./\nRUN npm ci\n\n# Copy source and build\nCOPY . .\nRUN npm run build\n\n# Stage 2: runtime\nFROM nginx:1.25-alpine\n\n# Copy built assets from build stage\nCOPY --from=build /app/dist /usr/share/nginx/html\n\nEXPOSE 80\nCMD [&quot;nginx&quot;,&quot;-g&quot;,&quot;daemon off;&quot;]\n\nWhat you achieve:\n\nNode, npm, devDependencies stay in the build stage.\nFinal image is just Nginx + static files.\nImage is smaller, faster to start, simpler to scan.\n\n\n3.2 Java: JDK + Maven → JRE‑only final\nYou saw a version of this earlier; here’s a focused multi‑stage example.\n# Stage 1: build\nFROM eclipse-temurin:21-jdk-alpine AS build\nWORKDIR /app\n\n# Dependencies\nCOPY pom.xml .\nRUN mvn -q -B dependency:go-offline\n\n# Source and build\nCOPY src ./src\nRUN mvn -q -B package -DskipTests\n\n# Stage 2: runtime\nFROM eclipse-temurin:21-jre-alpine\n\nWORKDIR /app\nCOPY --from=build /app/target/app.jar app.jar\n\nEXPOSE 8080\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\n\n\nProperties:\n\nJDK + Maven only in build stage.\nFinal runtime image has just a JRE and your JAR.\nMuch smaller than using maven:... directly as the only base.\n\nYou can then further harden the runtime stage (non‑root user, healthcheck, ENV, etc.).\n\n3.3 Go: build in golang, run in scratch or distroless\nGo produces static binaries easily, which are perfect for tiny runtime images.\n# Stage 1: build\nFROM golang:1.23-alpine AS build\nWORKDIR /app\n\nCOPY go.mod go.sum ./\nRUN go mod download\n\nCOPY . .\nRUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o app\n\n# Stage 2: runtime (scratch)\nFROM scratch\n\n# Copy binary and (optionally) CA certs if needed\nCOPY --from=build /app/app /app/app\n\n# Typically set a working dir and maybe certs or config\nWORKDIR /app\n\nENTRYPOINT [&quot;/app/app&quot;]\n\n\nAdvantages:\n\nFinal image includes only your compiled binary (and maybe CA certs).\nNo shell, no package manager, no extra libs.\nExtremely small, fast, and minimal attack surface.\n\nAlternatively, you can use a distroless base if you need some minimal runtime libs:\nFROM gcr.io/distroless/base\nCOPY --from=build /app/app /app/app\nWORKDIR /app\nENTRYPOINT [&quot;/app/app&quot;]\n\n4. Optimizing build cache in multi‑stage builds\nMulti‑stage doesn’t magically fix bad cache usage; you still need to order instructions smartly.\n4.1 General pattern: dependencies first, then source\nIn build stages for dependency‑heavy languages (Java, Node), do:\n\nCopy dependency manifest (pom.xml, package.json).\nInstall dependencies.\nCopy the rest of the source.\nBuild.\n\nJava example:\nFROM eclipse-temurin:21-jdk-alpine AS build\nWORKDIR /app\n\n# 1. Dependencies layer\nCOPY pom.xml .\nRUN mvn -q -B dependency:go-offline\n\n# 2. Source layer\nCOPY src ./src\nRUN mvn -q -B package -DskipTests\n\nNode example:\nFROM node:22-alpine AS build\nWORKDIR /app\n\nCOPY package*.json ./\nRUN npm ci\n\nCOPY . .\nRUN npm run build\n\nWhy this helps:\n\nAs long as your dependency file doesn’t change, the dependency install layer is reused.\nSmall code changes only invalidate the later layers, keeping builds much faster.\n\n4.2 Build‑only vs runtime files\nMulti‑stage gives you a natural separation:\n\nBuild stage: everything needed to compile/build (full source, tests, docs, etc.).\nRuntime stage: only what’s needed to run (binaries, JARs, static assets, configs).\n\nYou consciously choose what to copy with COPY --from, so:\n\nYou don’t accidentally ship test files or docs.\nYou don’t ship package lockfiles or build caches.\n\n\n5. Security and compliance benefits\nMulti‑stage builds are not just about size and speed; they directly improve security posture.\n5.1 Smaller attack surface\n\nNo compilers or interpreters in the runtime image.\nNo build tools like Maven, npm, pip, etc.\nFewer system libraries.\n\nIf an attacker breaks into the container, there’s less tooling to leverage for lateral movement or exploitation.\n5.2 Cleaner vulnerability scans\nSecurity tools scan all packages and libraries inside an image. If you ship a giant build image:\n\nYou see CVEs for compilers, build tools, and things that never run in production.\nNoise makes it harder to focus on real runtime risks.\n\nWith a minimal runtime image:\n\nScan results mostly reflect the libraries your app actually uses in production.\nRemediation and patching focus becomes much clearer.\n\n5.3 Easier upgrades\nWhen you separate build and runtime:\n\nYou can update the build image (new compiler, new Maven) without changing the runtime image base.\nOr update runtime base (for CVE patches) without touching build toolchain right away.\n\nThis decoupling can be helpful in regulated environments where change control for build and runtime flows differ.\n\n6. Migration story: from single‑stage to multi‑stage\nLet’s take a concrete, legacy single‑stage Dockerfile and evolve it step by step.\n6.1 Legacy single‑stage Java Dockerfile\nFROM maven:3.9-eclipse-temurin-21\n\nWORKDIR /app\nCOPY . .\nRUN mvn -q -B package -DskipTests\nEXPOSE 8080\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;target/app.jar&quot;]\n\nProblems:\n\nHeavy image: includes Maven and full JDK in production.\nCopies entire context (including target, .git, etc.).\nPoor cache usage (COPY . . early).\nNo separation of build vs runtime.\n\n6.2 Step 1: introduce build and runtime stages\n# Stage 1: build\nFROM maven:3.9-eclipse-temurin-21 AS build\nWORKDIR /app\nCOPY . .\nRUN mvn -q -B package -DskipTests\n\n# Stage 2: runtime\nFROM eclipse-temurin:21-jre-alpine\nWORKDIR /app\nCOPY --from=build /app/target/app.jar app.jar\nEXPOSE 8080\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\n\nNow:\n\nFinal image is based on JRE only.\nMaven &amp; JDK live only in the build stage.\n\n6.3 Step 2: improve cache usage in the build stage\nRefine the build stage:\nFROM maven:3.9-eclipse-temurin-21 AS build\nWORKDIR /app\n\n# Dependencies first\nCOPY pom.xml .\nRUN mvn -q -B dependency:go-offline\n\n# Then source\nCOPY src ./src\nRUN mvn -q -B package -DskipTests\n\nThis avoids re‑downloading dependencies on every code change.\n6.4 Step 3: slim down the builder base (optional)\nIf you want a lighter builder:\nFROM eclipse-temurin:21-jdk-alpine AS build\n# install Maven manually OR use a Maven wrapper\nWORKDIR /app\n\nCOPY mvnw pom.xml ./\nCOPY .mvn .mvn\nRUN ./mvnw -q -B dependency:go-offline\n\nCOPY src ./src\nRUN ./mvnw -q -B package -DskipTests\n\nBuild still happens in build stage, but with a smaller JDK + Maven wrapper instead of the maven: base.\n6.5 Step 4: harden the runtime stage\nNow secure the runtime stage as well:\nFROM eclipse-temurin:21-jre-alpine\n\n# Create non-root user\nRUN addgroup -S app &amp;&amp; adduser -S app -G app\n\nWORKDIR /app\nCOPY --from=build /app/target/app.jar app.jar\n\nENV SPRING_PROFILES_ACTIVE=prod\n\nUSER app\nEXPOSE 8080\n\nHEALTHCHECK --interval=30s --timeout=3s --retries=3 \\\n  CMD curl -f http://localhost:8080/actuator/health || exit 1\n\nENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\nCMD [&quot;--spring.profiles.active=prod&quot;]\n\n\nFinal result:\n\nMulti‑stage build.\nOptimized caching.\nMinimal runtime image.\nNon‑root user.\nHealthcheck and ENV defaults.\n\nThe behavior (run a Spring Boot JAR) is the same, but the image is faster, smaller, and safer."},"Docker/7-Docker-Security-Essentials":{"slug":"Docker/7-Docker-Security-Essentials","filePath":"Docker/7 Docker Security Essentials.md","title":"7. Docker Security Essentials","links":[],"tags":[],"content":"Containers make deployment easier, but they don’t magically make things safe. A container is just a process on a shared kernel with some isolation. Security comes from how you build images and how you run containers, not from Docker’s existence alone.\n\n1. Threat model for containers\nShared kernel: what it implies\n\nContainers are not VMs: there is no separate kernel per container.\nAll containers and the host share the same Linux kernel, using:\n\nNamespaces for isolation (PID, network, mount, etc.).\ncgroups for resource limits.\n\n\n\nImplications:\n\nA kernel bug or misconfiguration can potentially be exploited from inside a container.\nIf a container breaks out of its namespaces, it may affect other containers or the host.\n\n“It’s in a container” is not a magic boundary\nCommon misconceptions:\n\n“If it’s in a container, it’s safe even if running as root.”\n“We can just mount anything from the host; Docker will protect us.”\n\nReality:\n\nRoot inside a container can often do dangerous things if the container is misconfigured (e.g., privileged mode, host mounts).\nVolumes and bind mounts can expose sensitive host paths directly.\nDocker is a convenience, not a security sandbox; treat containers as apps running on the host, with extra isolation but not perfect.\n\nMindset:\nAssume an attacker can get code execution inside the container. Your job is to limit what they can do from there.\n\n2. User and permissions\nNon‑root users inside containers\nBy default, many base images run as root. If your app is compromised, the attacker gets root inside the container.\nBetter pattern:\n\nCreate a dedicated user in the Dockerfile.\nSwitch to it with USER.\n\nExample:\nFROM eclipse-temurin:21-jre-alpine\n\n# Create group and user\nRUN addgroup -S app &amp;&amp; adduser -S app -G app\n\nWORKDIR /app\nCOPY app.jar /app/app.jar\n\nUSER app\n\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\n\n\nBenefits:\n\nThe app runs with limited privileges.\nEven if an exploit lands, it can’t directly perform many root‑level operations.\n\nFile ownership with volumes and bind mounts\nVolumes and bind mounts introduce another dimension: who owns the files.\nTypical issues:\n\nHost directory owned by a user with UID/GID that doesn’t match the container’s user.\nContainer process gets “permission denied” when trying to write.\n\nPatterns:\n\nAlign UIDs: run container with a user whose UID/GID matches host directory owner, or adjust host directory permissions.\nLet the container initialize its own volume:\n\nFirst run as root to set ownership, then drop to non‑root, or\nUse an init container step (in orchestration) to prepare permissions.\n\n\n\nExample volume pattern:\ndocker run -d --name db \\\n  -e POSTGRES_PASSWORD=secret \\\n  -v pgdata:/var/lib/postgresql/data \\\n  postgres:15-alpine\n\nHere, Postgres image is usually prepared to manage its own data directory permissions. For your custom images, make sure the USER has write permission to any mounted paths.\n\n3. Capabilities and seccomp (conceptual)\nLinux capabilities: fine‑grained root\nIn Linux, “root” privileges are split into capabilities (e.g., NET_ADMIN, SYS_ADMIN). Docker containers get a reduced set of capabilities by default, but often more than they strictly need.\nInstead of all‑or‑nothing root, you can drop capabilities and only add back what’s required.\nExample pattern:\ndocker run -d --name web \\\n  --cap-drop=ALL \\\n  --cap-add=NET_BIND_SERVICE \\\n  -p 80:80 \\\n  nginx:1.25-alpine\n\n\n\n--cap-drop=ALL removes all capabilities.\n--cap-add=NET_BIND_SERVICE permits binding to low ports (&lt;1024) without full root.\n\nThis is a powerful “least privilege” control.\nSeccomp (high level)\nSeccomp (secure computing mode) lets you filter which syscalls a process can make. Docker ships with a default seccomp profile that blocks some risky syscalls.\nYou rarely write seccomp profiles by hand at first, but you should know that:\n\nDocker can block dangerous syscalls by default.\nYou can opt into different profiles or, in rare cases, relax them if your app needs unusual syscalls.\n\nBasic principle:\nDon’t disable the default seccomp profile unless you really know what you’re doing.\n\n4. Image hygiene\nSecurity also comes from what’s inside your image.\nKeep base images updated\n\nBase images inherit vulnerabilities from the underlying OS and libraries.\nYou should periodically:\n\nRebuild images from updated base tags.\nTrack upstream base image changelogs and security advisories.\n\n\n\nUsing versioned tags like eclipse-temurin:21-jre-alpine is good, but you still need to pull updated variants as they are released and rebuild your images.\nInstall only what you need, clean up after\nEvery extra package:\n\nIncreases attack surface.\nIncreases the number of things scanners will flag.\n\nPatterns:\n\nAvoid “kitchen sink” installs (apt-get install -y nano vim curl wget net-tools ...) in runtime images.\nUse minimal installs and remove package caches.\n\nExample for Debian/Ubuntu based images:\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends curl &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\nFor Alpine:\nRUN apk add --no-cache curl\nCombine install and cleanup in one RUN so caches don’t remain in previous layers.\n\n5. Runtime hardening flags\nBeyond the Dockerfile, run‑time options give you strong guardrails.\nRead‑only root filesystem\nIf your app doesn’t need to write to the root filesystem, you can mount it read‑only and use volumes for writable paths.\ndocker run -d --name api \\\n  --read-only \\\n  -v app-tmp:/tmp \\\n  myorg/api:1.0.0\n\n\nEffects:\n\nThe container’s root filesystem is read‑only.\nApp can still write to /tmp (a dedicated volume).\nAttacks that try to drop binaries or modify configuration files on root FS are harder.\n\nNo new privileges\n--security-opt no-new-privileges prevents processes from gaining more privileges (e.g., via setuid binaries).\ndocker run -d --name api \\\n  --security-opt no-new-privileges \\\n  myorg/api:1.0.0\n\n\nThis enforces a strong “no escalation” rule even if some binaries have setuid bits.\nTightening Nginx as an example\nPutting it together:\ndocker run -d --name web \\\n  -p 80:80 \\\n  --read-only \\\n  -v web-logs:/var/log/nginx \\\n  --cap-drop=ALL \\\n  --cap-add=NET_BIND_SERVICE \\\n  --security-opt no-new-privileges \\\n  nginx:1.25-alpine\n\nWe’ve:\n\nDropped all capabilities except what’s needed to bind port 80.\nMade filesystem read‑only except for logs.\nPrevented privilege escalation.\n\nThis is a realistic baseline for a simple web server.\n\n6. Secrets handling basics\nWhy not bake secrets into images\nBad patterns:\n\nENV DB_PASSWORD=supersecret in the Dockerfile.\nCopying .env or config with credentials into the image.\nChecking these files into source control.\n\nProblems:\n\nSecret is now in every environment where the image lands.\nAnyone with access to the image registry can potentially read it.\nRotating the secret requires rebuilding images.\n\nBetter approaches (high‑level)\nThe principle: inject secrets at runtime, not build time.\nCommon patterns:\n\nRuntime environment variables:\n\ndocker run -e DB_PASSWORD=...\nBut still sensitive; use restricted environment (orchestrator secrets) rather than plain text in scripts.\n\n\nOrchestrator secret mechanisms:\n\nDocker Swarm secrets.\nKubernetes Secrets (mounted as files or env vars).\nCloud provider secret managers (AWS Secrets Manager, GCP Secret Manager, Vault, etc.).\n\n\n\nIn all cases:\n\nThe Dockerfile should never hardcode secrets.\nImages should be reusable across environments; secrets are part of deployment configuration, not the artifact itself.\n\n\nPutting it all together: baseline security checklist\nWhen you design Docker images and containers, treat this as your default checklist:\n\nThreat model:\n\nRemember containers share the host kernel; don’t assume perfect containment.\n\n\nUsers &amp; permissions:\n\nCreate a non‑root user in Dockerfile, USER to it.\nEnsure volumes and bind mounts respect that user’s permissions.\n\n\nCapabilities &amp; seccomp:\n\nDrop capabilities by default, add only what you need.\nDon’t casually disable Docker’s default seccomp profile.\n\n\nImage hygiene:\n\nUse minimal, versioned base images.\nInstall only necessary packages; clean caches.\nRebuild on base image updates.\n\n\nRuntime hardening:\n\nUse --read-only and dedicated writable volumes where possible.\nApply --security-opt no-new-privileges.\nAvoid unnecessary privileged containers and host mounts.\n\n\nSecrets:\n\nNever bake secrets into the image.\nUse runtime injection and secret management systems.\n\n\n\nOnce this baseline becomes automatic, you’ll naturally design Docker setups that are much closer to what SREs and security teams expect in real production environments."},"Docker/8-Docker-Registries-and-Tagging":{"slug":"Docker/8-Docker-Registries-and-Tagging","filePath":"Docker/8 Docker Registries and Tagging.md","title":"8. Docker Registries and Tagging","links":[],"tags":[],"content":"Containers are just a packaging format; the real control over what runs in each environment comes from your registry and tagging strategy. If you treat images like real versioned artifacts (not random blobs named “latest”), debugging, rollbacks, and audits become much simpler.\n\n1. Mental model: registry as artifact repository\nThink of a container registry like a Maven repository or Git server, but for images.\n\nPublic registries\n\nDocker Hub (docker.io/library/nginx).\nPublic GHCR (GitHub Container Registry), etc.\nGreat for base images and open‑source artifacts.\n\n\nPrivate registries\n\nAWS ECR (123456789012.dkr.ecr.us-east-1.amazonaws.com/myorg/api).\nGCP Artifact Registry, Azure ACR.\nSelf‑hosted: Harbor, GitLab Registry, etc.\nUsed for your org’s internal services.\n\n\n\nAn image name has three parts:\n[REGISTRY/]NAMESPACE/REPOSITORY:TAG\nExamples:\n\nnginx:1.25-alpine\n\nRegistry: default Docker Hub.\nNamespace: library (implicit).\nRepository: nginx.\nTag: 1.25-alpine.\n\n\nmy-registry.example.com/myorg/payment-service:1.0.0\n\nRegistry: my-registry.example.com.\nNamespace: myorg.\nRepository: payment-service.\nTag: 1.0.0.\n\n\n\nNamespace + repository (myorg/service) should reflect ownership and purpose, like Maven group/artifact.\n\n2. Tagging strategies that don’t ruin your life\nTags are labels pointing to an image ID. They’re not immutable by themselves; you decide how to use them.\nSemantic versions and build numbers\nTreat images like versioned releases:\n\nSemantic versions: 1.0.0, 1.0.1, 2.0.0.\nBuild identifiers: 1.0.0-20260201.1, 1.0.0+build.42.\nGit SHA tags: app:git-abc1234.\n\nCommon pattern after building an image from commit abc1234:\ndocker tag myorg/api:build \\\n  myorg/api:1.2.3 \\\n  myorg/api:1.2 \\\n  myorg/api:git-abc1234\n\n\nNow you can refer to the exact artifact by any of these tags, but git-abc1234 is uniquely tied to a commit.\nUsing latest responsibly\nlatest is just another tag; Docker doesn’t treat it specially. Problems arise when:\n\nDifferent teams assume latest means different things.\nlatest points to different builds across environments.\n\nRecommended:\n\nIn production, avoid using latest in deployments. Use explicit version tags.\nIf you keep latest at all, treat it as “most recent successful stable build,” and be disciplined about how it’s updated.\n\nMapping dev / staging / prod to tags\nInstead of rebuilding for each environment, use tags to represent promotion level:\n\nmyorg/api:1.2.3-dev\nmyorg/api:1.2.3-staging\nmyorg/api:1.2.3-prod\n\nOr keep environment tags separate from version tags:\n\nmyorg/api:1.2.3 is the version.\nmyorg/api:prod points to whatever version is currently live in production (you move the prod tag during promotion).\n\nThis lets you answer:\n\n“What version is in prod?” → inspect myorg/api:prod.\n“What’s running in staging?” → myorg/api:staging.\n\n\n3. Push/pull workflow in CI/CD\nA clean CI/CD flow treats image building as part of the pipeline and relies on tagging for traceability.\nExample pipeline flow\nGiven a commit abc1234 for version 1.2.3:\n\nBuild image\ndocker build -t myorg/api:build .\nTag image with metadata\n\nGIT_SHA=abc1234\nVERSION=1.2.3\n\ndocker tag myorg/api:build myorg/api:${VERSION}\ndocker tag myorg/api:build myorg/api:${VERSION}-${GIT_SHA}\ndocker tag myorg/api:build myorg/api:git-${GIT_SHA}\n\n\n\nPush to registry\n\ndocker push myorg/api:${VERSION}\ndocker push myorg/api:${VERSION}-${GIT_SHA}\ndocker push myorg/api:git-${GIT_SHA}\n\n\nDeploy by tag\n\nDev: deploy myorg/api:${VERSION}-${GIT_SHA}.\nStaging: when tests pass, deploy the same tag.\nProd: promote that tag again.\n\n\n\nAt no point do you rebuild the image for each environment; you always deploy the same artifact.\nWhy this is powerful\n\nEvery running container is traceable back to a Git SHA and build.\nLogs, metrics, and incidents can be tied to a specific image version.\nWhen a bug is found, you know exactly which version introduced it.\n\n\n4. Promoting images across environments\nA common anti‑pattern: build separate images for dev, staging, prod, even when code is identical.\nBetter: build once, promote via tags.\nPromotion via retagging\nStart with myorg/api:1.2.3-abc1234 as your canonical build tag.\n\nDev deployment: use myorg/api:1.2.3-abc1234.\nOnce validated, retag in the registry or via CI:\n\ndocker tag myorg/api:1.2.3-abc1234 myorg/api:staging\ndocker push myorg/api:staging\n\n\n\nFor production, do:\n\ndocker tag myorg/api:1.2.3-abc1234 myorg/api:prod\ndocker push myorg/api:prod\n\nNow:\n\nprod represents “current production image.”\nstaging represents “current staging image.”\nBoth point back to the same underlying build and Git SHA.\n\nBenefits for debugging and rollback\nWhen something breaks in prod:\n\nYou can quickly see which tag/version is running.\nTo roll back, point prod back to a previous known-good tag:\n\ndocker tag myorg/api:1.2.2-xyz9876 myorg/api:prod\ndocker push myorg/api:prod\n\nYour deployment tooling then picks up the updated prod tag and redeploys, without rebuilding.\nThis is very similar to promoting artifacts through environments in Maven or artifact repositories, but with container images.\n\n5. Registry access and auth basics\nLogging into registries\nFor private registries, you authenticate before pushing or pulling:\ndocker login my-registry.example.com\n# prompts for username/password or token\n\n\nIn CI:\n\nUse access tokens or service accounts instead of personal credentials.\nConfigure credentials as pipeline secrets and inject them into the build job.\n\nImage names in orchestrators\nWhen you use Docker Compose, Swarm, or Kubernetes, you reference the same image names and tags:\n\nCompose:\n\nservices:\n  api:\n    image: my-registry.example.com/myorg/api:1.2.3-abc1234\n\n\nKubernetes Deployment:\n\nspec:\n  template:\n    spec:\n      containers:\n      - name: api\n        image: my-registry.example.com/myorg/api:1.2.3-abc1234\n\nThe orchestrator:\n\nAuthenticates against the registry (via imagePullSecrets or node IAM roles).\nPulls the specified image tag onto the nodes.\n\nSo your tagging strategy directly affects:\n\nReproducibility of deployments.\nHow easy it is to roll back, audit, and debug.\n\n\nPractical checklist for registries &amp; tagging\nWhen designing your registry/tagging setup, aim for:\n\nConsistent naming: registry/namespace/service:tag with clear org/service mapping.\nVersioned tags: semantic versions plus build/Git SHA tags.\nMinimal use of latest: avoid it entirely in production manifests.\nPromotion via tags, not rebuilds: build once, promote the same artifact through dev → staging → prod.\nTraceability: every running container’s image tag lets you find the exact Git commit and build.\n"},"Docker/9-Docker-Compose":{"slug":"Docker/9-Docker-Compose","filePath":"Docker/9 Docker Compose.md","title":"9. Docker Compose","links":[],"tags":[],"content":"Running one container with docker run is fine. Running four containers (API, DB, cache, UI) with long commands and manual networks is pain. Docker Compose solves this by letting you describe your whole stack in a single YAML file and manage it with a few short commands.\n\n1. Why Compose exists\nThe pain of raw docker run\nA realistic microservice stack might need for each container:\n\n--name\n-p port mappings\n--network\n-e environment variables\n-v volumes\n\nFor an api + db stack, that’s already two ugly commands you must remember and retype, and it gets much worse with more services. Recreating the same environment on another machine is error‑prone.\nYAML as “docker run on steroids”\nDocker Compose lets you write a declarative description of your stack in docker-compose.yml:\n\nEach service describes its image, ports, environment, volumes, networks.\nCompose takes care of:\n\nCreating the network(s).\nStarting services in the right order (with depends_on as a hint).\nWiring DNS names (service names) for container‑to‑container communication.\n\n\n\nOne file becomes your local environment definition that you can version‑control and share.\n\n2. Core concepts\nAt a high level, Compose YAML has three main sections you use most often:\n\nservices – each container you want to run.\nvolumes – named volumes for persistent storage.\nnetworks – logical networks connecting services.\n\nExample skeleton:\nversion: &quot;3.9&quot;\n\nservices:\n  api:\n    image: myorg/orders-api:1.0.0\n    ports:\n      - &quot;8080:8080&quot;\n    environment:\n      DB_HOST: db\n\n  db:\n    image: postgres:15-alpine\n    environment:\n      POSTGRES_PASSWORD: secret\n    volumes:\n      - pgdata:/var/lib/postgresql/data\n\nvolumes:\n  pgdata:\n\nnetworks:\n  # optional to declare explicitly; Compose creates a default one if omitted\n\nMental model:\n\nservices.api and services.db are like named docker run definitions.\nvolumes.pgdata is like docker volume create pgdata.\nCompose automatically creates a dedicated network for this stack, and connects all services to it.\n\nOne docker-compose.yml = one stack (your “local environment”).\n\n3. Walking through a simple stack\nLet’s build a concrete api + db stack.\nCompose file\nversion: &quot;3.9&quot;\n\nservices:\n  db:\n    image: postgres:15-alpine\n    container_name: orders-db\n    environment:\n      POSTGRES_DB: orders\n      POSTGRES_USER: orders_user\n      POSTGRES_PASSWORD: secret\n    volumes:\n      - pgdata:/var/lib/postgresql/data\n    ports:\n      - &quot;5432:5432&quot;\n\n  api:\n    image: myorg/orders-api:1.0.0\n    container_name: orders-api\n    depends_on:\n      - db\n    environment:\n      DB_HOST: db        # service name, not host IP\n      DB_PORT: 5432\n      DB_NAME: orders\n      DB_USER: orders_user\n      DB_PASSWORD: secret\n    ports:\n      - &quot;8080:8080&quot;\n\nvolumes:\n  pgdata:\n\nWhat Compose does when you run docker compose up:\n\nCreates a network (e.g., foldername_default).\nStarts db on that network with hostname db.\nStarts api on the same network with hostname api.\nSets env vars inside each container as defined.\nPublishes host ports:\n\nHost 5432 → container db:5432.\nHost 8080 → container api:8080.\n\n\n\nService‑to‑service communication:\n\napi reaches the database using DB_HOST=db (Docker‑provided DNS name).\nYou don’t care about the actual container IPs.\n\nFrom your host:\n\npsql -h localhost -p 5432 -U orders_user orders\ncurl http://localhost:8080/actuator/health\n\nThis is exactly the networking and volumes mental model you already built, but declared in YAML instead of manual docker run flags.\n\n4. Developer workflows\nCompose gives you a few key commands that cover almost all daily needs.\nAssume you have docker-compose.yml in the current directory.\n4.1 Bring up the stack\ndocker compose up -d\n\n-d runs in detached mode.\nCreates the network and volumes if they don’t exist.\nStarts all services.\n\nTo see what’s running:\ndocker compose ps\n4.2 Logs\nTo see logs for all services:\ndocker compose logs\n# or follow:\ndocker compose logs -f\n\nFor just the API:\nbash\ndocker compose logs -f api\n4.3 Restarting services\nIf you’ve rebuilt the image or changed configuration:\nbash\ndocker compose restart api\nThis restarts only the api service, leaving db and others alone.\nWhen code changes and you rebuild the image:\ndocker build -t myorg/orders-api:1.0.1 .\ndocker compose up -d api\n\nCompose compares the new image and recreates just that service.\n4.4 Stopping and cleaning up\nTo stop containers but keep volumes/networks:\nbash\ndocker compose down\nTo also remove volumes (careful: data loss for DB):\nbash\ndocker compose down -v\nIn dev:\n\nUse docker compose down when you just want to stop the stack.\nUse -v when you want a completely fresh environment (fresh DB, etc.).\n\n\n5. Patterns and best practices\n5.1 Separate override files for local dev vs CI\nCompose supports multiple files:\n\nBase file: docker-compose.yml (common definition).\nOverrides: docker-compose.override.yml, docker-compose.dev.yml, etc.\n\nExample:\ndocker-compose.yml (base):\nservices:\n  api:\n    image: myorg/orders-api:1.0.0\n    environment:\n      DB_HOST: db\n  db:\n    image: postgres:15-alpine\n    environment:\n      POSTGRES_PASSWORD: secret\n    volumes:\n      - pgdata:/var/lib/postgresql/data\n\nvolumes:\n  pgdata:\n\n\ndocker-compose.override.yml (local dev specifics):\nservices:\n  api:\n    build: .\n    image: myorg/orders-api:dev\n    ports:\n      - &quot;8080:8080&quot;\n  db:\n    ports:\n      - &quot;5432:5432&quot;\n\nThen:\ndocker compose up -d\nautomatically applies both base and override. In CI, you might use only the base file or a different override (e.g., no host port exposure).\nThis pattern:\n\nKeeps environment‑independent config in one place.\nKeeps environment‑specific tweaks (ports, build vs image, debug tools) in overrides.\n\n5.2 Healthchecks in Compose for better startup order\nCompose’s depends_on only ensures start order, not readiness. Your DB may start its process but not yet be ready to accept connections.\nYou can define healthchecks at the service level:\nservices:\n  db:\n    image: postgres:15-alpine\n    environment:\n      POSTGRES_PASSWORD: secret\n    healthcheck:\n      test: [&quot;CMD-SHELL&quot;,&quot;pg_isready -U postgres&quot;]\n      interval: 10s\n      timeout: 3s\n      retries: 5\n\n  api:\n    image: myorg/orders-api:1.0.0\n    depends_on:\n      db:\n        condition: service_healthy\n\nNow:\n\nCompose waits until db is marked healthy before starting api.\nThis avoids common “API can’t connect to DB on startup” races in local dev.\n\n5.3 Avoid re‑encoding docker run flags\nLet Compose own most of the configuration:\n\nPut ports, env, volumes, networks into YAML, not CLI flags.\nUse docker compose commands instead of docker run for those services.\n\ndocker run is still fine for one‑off debug containers (alpine shells, tools). For your stack, Compose should be the source of truth.\n\n6. Bridge to Kubernetes\nCompose is conceptually very close to how you define workloads in Kubernetes; the vocabulary changes, but the mental model stays.\nMapping:\n\nCompose service → Kubernetes Deployment (or StatefulSet) + Service.\nCompose volumes → Kubernetes PersistentVolumes and PersistentVolumeClaims.\nCompose networks → Kubernetes cluster network and Service discovery (DNS names).\nCompose environment variables → Kubernetes Pod env vars.\n\nExample concept mapping for api + db:\n\nservices.api:\n\nimage: myorg/orders-api:1.0.0 → Deployment.spec.template.spec.containers[0].image.\nports: &quot;8080:8080&quot; → Service exposing port 8080 externally.\nenvironment → Pod env vars.\n\n\nservices.db:\n\nvolumes: pgdata:/var/lib/postgresql/data → PVC + volume mount.\nhealthcheck → liveness/readiness probes.\n\n\n\nSo by getting comfortable with:\n\nDeclaring services, volumes, and networks in YAML.\nUsing service names instead of IPs.\nManaging multi‑container lifecycles with a few commands.\n\nyou’re building intuition that transfers almost 1:1 into Kubernetes manifests and Helm charts later."},"index":{"slug":"index","filePath":"index.md","title":"Docker Mastery","links":["Docker/0-Course-Overview-and-Index","Docker/1-Docker-Images","Docker/2-Docker-Containers","Docker/3-Docker-Networking","Docker/4-Docker-Volumes","Docker/5-Dockerfile-Mastery","Docker/6-Multi‑Stage-Docker-Builds","Docker/7-Docker-Security-Essentials","Docker/8-Docker-Registries-and-Tagging","Docker/9-Docker-Compose","Docker/10-Debugging-Docker","Docker/11-From-Docker-to-Orchestrators"],"tags":[],"content":"\nDocker Mastery is a structured learning track and technical reference for\r\ndevelopers and DevOps engineers who want a deep, practical understanding of\r\nDocker.\nThe material is organized as a guided course, progressing from core concepts\r\n(images and containers) to networking, storage, Compose, and\r\norchestration-readiness. At the same time, each module is written to stand on its\r\nown, making the content useful as day-to-day documentation and reference.\nThe focus is on mental models, real-world behavior, and operational trade-offs\r\nrather than memorizing commands. Examples and best practices reflect how Docker\r\nis used in real systems, not just local demos.\nYou can follow the modules sequentially for a complete learning path, or jump\r\ndirectly to specific topics as needed.\nModules\n\n\nCourse Overview and Index\n\n\nDocker Images\n\n\nDocker Containers\n\n\nDocker Networking\n\n\nDocker Volumes\n\n\nDockerfile Mastery\n\n\nMulti‑Stage Docker Builds\n\n\nDocker Security Essentials\n\n\nDocker Registries and Tagging\n\n\nDocker Compose\n\n\nDebugging Docker\n\n\nFrom Docker to Orchestrators – How Your Mental Models Carry Over\n\n"}}